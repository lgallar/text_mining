{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WwyUKbRHQOV"
   },
   "source": [
    "# Working with embedings using Gensim\n",
    "- Train word2vect embedings from a corpus\n",
    "- Load pretrained embedings\n",
    "- Use embedings to Classify text\n",
    "- Accest to the embedings of spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "Hy4dhVLUHQOW",
    "outputId": "1874197b-79cb-4f66-ca9a-97b034780b9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: paramiko in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /usr/local/lib/python3.6/dist-packages (from paramiko) (3.1.7)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from paramiko) (1.3.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.6/dist-packages (from paramiko) (2.8)\n",
      "Requirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.6/dist-packages (from bcrypt>=3.1.3->paramiko) (1.13.1)\n",
      "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from bcrypt>=3.1.3->paramiko) (1.12.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko) (2.19)\n",
      "Gensim Version:  3.6.0\n"
     ]
    }
   ],
   "source": [
    "# Header\n",
    "import os\n",
    "\n",
    "\n",
    "# solve gensim warning\n",
    "! pip install paramiko\n",
    "\n",
    "import gensim, logging\n",
    "print('Gensim Version: ', gensim.__version__)\n",
    "\n",
    "\n",
    "# Data path\n",
    "data_path = '.'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "avJaU3apHQOa",
    "outputId": "bdfd90a7-71e9-499a-89d9-ce88b96a578d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-29 10:04:57,232 : INFO : collecting all words and their counts\n",
      "2019-10-29 10:04:57,233 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-10-29 10:04:57,234 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2019-10-29 10:04:57,235 : INFO : Loading a fresh vocabulary\n",
      "2019-10-29 10:04:57,236 : INFO : effective_min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2019-10-29 10:04:57,238 : INFO : effective_min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2019-10-29 10:04:57,239 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2019-10-29 10:04:57,240 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2019-10-29 10:04:57,241 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2019-10-29 10:04:57,242 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2019-10-29 10:04:57,243 : INFO : resetting layer weights\n",
      "2019-10-29 10:04:57,246 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-10-29 10:04:57,249 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-29 10:04:57,250 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-29 10:04:57,251 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-29 10:04:57,252 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-10-29 10:04:57,255 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-29 10:04:57,256 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-29 10:04:57,257 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-29 10:04:57,258 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-10-29 10:04:57,261 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-29 10:04:57,263 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-29 10:04:57,264 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-29 10:04:57,265 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 236 effective words/s\n",
      "2019-10-29 10:04:57,268 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-29 10:04:57,268 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-29 10:04:57,270 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-29 10:04:57,271 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-10-29 10:04:57,273 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-29 10:04:57,274 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-29 10:04:57,275 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-29 10:04:57,276 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-10-29 10:04:57,277 : INFO : training on a 20 raw words (1 effective words) took 0.0s, 33 effective words/s\n",
      "2019-10-29 10:04:57,278 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# Example code to build a word2vect embedings from a corpus\n",
    "\n",
    "# To show in the output the internal messages of the word2vect process\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "# My little corpus    \n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vqsR1EKaHQOc",
    "outputId": "c349fc6d-00d8-4f63-8d05-1bfe06e390ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Corpus sentences len: 57340\n",
      "Corpus words len: 1161192\n"
     ]
    }
   ],
   "source": [
    "# Load a more big corpus\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "print('Corpus sentences len:', len(brown.sents()))\n",
    "print('Corpus words len:', len(brown.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6GA4HkmlHQOf",
    "outputId": "833091a2-52c5-4aa7-d843-096129c5fbb2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-29 10:05:01,529 : INFO : collecting all words and their counts\n",
      "2019-10-29 10:05:01,531 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-10-29 10:05:02,002 : INFO : PROGRESS: at sentence #10000, processed 219770 words, keeping 23488 word types\n",
      "2019-10-29 10:05:02,436 : INFO : PROGRESS: at sentence #20000, processed 430477 words, keeping 34367 word types\n",
      "2019-10-29 10:05:02,888 : INFO : PROGRESS: at sentence #30000, processed 669056 words, keeping 42365 word types\n",
      "2019-10-29 10:05:03,318 : INFO : PROGRESS: at sentence #40000, processed 888291 words, keeping 49136 word types\n",
      "2019-10-29 10:05:03,661 : INFO : PROGRESS: at sentence #50000, processed 1039920 words, keeping 53024 word types\n",
      "2019-10-29 10:05:03,932 : INFO : collected 56057 word types from a corpus of 1161192 raw words and 57340 sentences\n",
      "2019-10-29 10:05:03,933 : INFO : Loading a fresh vocabulary\n",
      "2019-10-29 10:05:03,977 : INFO : effective_min_count=5 retains 15173 unique words (27% of original 56057, drops 40884)\n",
      "2019-10-29 10:05:03,978 : INFO : effective_min_count=5 leaves 1095086 word corpus (94% of original 1161192, drops 66106)\n",
      "2019-10-29 10:05:04,029 : INFO : deleting the raw counts dictionary of 56057 items\n",
      "2019-10-29 10:05:04,032 : INFO : sample=0.001 downsamples 42 most-common words\n",
      "2019-10-29 10:05:04,033 : INFO : downsampling leaves estimated 781596 word corpus (71.4% of prior 1095086)\n",
      "2019-10-29 10:05:04,075 : INFO : estimated required memory for 15173 words and 100 dimensions: 19724900 bytes\n",
      "2019-10-29 10:05:04,076 : INFO : resetting layer weights\n",
      "2019-10-29 10:05:06,862 : INFO : training model with 4 workers on 15173 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-10-29 10:05:07,880 : INFO : EPOCH 1 - PROGRESS: at 24.43% examples, 197728 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:08,899 : INFO : EPOCH 1 - PROGRESS: at 48.67% examples, 203541 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:09,925 : INFO : EPOCH 1 - PROGRESS: at 76.21% examples, 208515 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:10,633 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-29 10:05:10,635 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-29 10:05:10,641 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-29 10:05:10,648 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-29 10:05:10,649 : INFO : EPOCH - 1 : training on 1161192 raw words (781703 effective words) took 3.8s, 206771 effective words/s\n",
      "2019-10-29 10:05:11,663 : INFO : EPOCH 2 - PROGRESS: at 25.39% examples, 205695 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:12,671 : INFO : EPOCH 2 - PROGRESS: at 50.16% examples, 211924 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:13,675 : INFO : EPOCH 2 - PROGRESS: at 77.47% examples, 213379 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:14,337 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-29 10:05:14,339 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-29 10:05:14,346 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-29 10:05:14,355 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-29 10:05:14,355 : INFO : EPOCH - 2 : training on 1161192 raw words (782023 effective words) took 3.7s, 211339 effective words/s\n",
      "2019-10-29 10:05:15,373 : INFO : EPOCH 3 - PROGRESS: at 25.39% examples, 204523 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:16,375 : INFO : EPOCH 3 - PROGRESS: at 49.43% examples, 208794 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:17,387 : INFO : EPOCH 3 - PROGRESS: at 75.14% examples, 208334 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:18,141 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-29 10:05:18,145 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-29 10:05:18,150 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-29 10:05:18,159 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-29 10:05:18,161 : INFO : EPOCH - 3 : training on 1161192 raw words (781609 effective words) took 3.8s, 205669 effective words/s\n",
      "2019-10-29 10:05:19,185 : INFO : EPOCH 4 - PROGRESS: at 22.82% examples, 182773 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:20,197 : INFO : EPOCH 4 - PROGRESS: at 47.04% examples, 196745 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:21,197 : INFO : EPOCH 4 - PROGRESS: at 70.81% examples, 198813 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:22,138 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-29 10:05:22,143 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-29 10:05:22,149 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-29 10:05:22,154 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-29 10:05:22,155 : INFO : EPOCH - 4 : training on 1161192 raw words (781531 effective words) took 4.0s, 195828 effective words/s\n",
      "2019-10-29 10:05:23,177 : INFO : EPOCH 5 - PROGRESS: at 22.82% examples, 184167 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:24,182 : INFO : EPOCH 5 - PROGRESS: at 45.87% examples, 191616 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:25,187 : INFO : EPOCH 5 - PROGRESS: at 68.78% examples, 195158 words/s, in_qsize 0, out_qsize 0\n",
      "2019-10-29 10:05:26,136 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-29 10:05:26,139 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-29 10:05:26,146 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-29 10:05:26,147 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-29 10:05:26,148 : INFO : EPOCH - 5 : training on 1161192 raw words (782022 effective words) took 4.0s, 196224 effective words/s\n",
      "2019-10-29 10:05:26,149 : INFO : training on a 5805960 raw words (3908888 effective words) took 19.3s, 202691 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# Train a word2 vect model over the new corpus\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(brown.sents(), size=100, window=5, min_count=5, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "xCfe5WxWHQOi",
    "outputId": "a7e18966-bdc7-4390-dabc-303d5c0c4f3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-29 10:05:26,160 : INFO : saving Word2Vec object under brown_word2vect_model.bin, separately None\n",
      "2019-10-29 10:05:26,162 : INFO : not storing attribute vectors_norm\n",
      "2019-10-29 10:05:26,163 : INFO : not storing attribute cum_table\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-10-29 10:05:26,345 : INFO : saved brown_word2vect_model.bin\n"
     ]
    }
   ],
   "source": [
    "#Persist the model\n",
    "\n",
    "model.save('brown_word2vect_model.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "n56mr7uGHQOk",
    "outputId": "c40cc4e2-30b7-4646-f5b2-9f5fbf4f0437"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-29 10:05:26,352 : INFO : loading Word2Vec object from brown_word2vect_model.bin\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-10-29 10:05:26,484 : INFO : loading wv recursively from brown_word2vect_model.bin.wv.* with mmap=None\n",
      "2019-10-29 10:05:26,486 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-10-29 10:05:26,486 : INFO : loading vocabulary recursively from brown_word2vect_model.bin.vocabulary.* with mmap=None\n",
      "2019-10-29 10:05:26,492 : INFO : loading trainables recursively from brown_word2vect_model.bin.trainables.* with mmap=None\n",
      "2019-10-29 10:05:26,494 : INFO : setting ignored attribute cum_table to None\n",
      "2019-10-29 10:05:26,495 : INFO : loaded brown_word2vect_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Load a trained model\n",
    "\n",
    "model = Word2Vec.load('brown_word2vect_model.bin')  # you can continue training with the loaded model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "SB57OZBXHQOn",
    "outputId": "180346fa-ef61-41ee-f955-fbcedf2951b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6654373 , -0.46135205,  0.38971207, -0.04051128,  0.36708117,\n",
       "       -0.40918276, -0.5273041 , -0.23855475,  0.7257526 ,  1.1274878 ,\n",
       "       -0.392828  ,  0.8888476 ,  0.26686767,  0.07012271,  1.4807639 ,\n",
       "        0.3807536 ,  1.2558132 ,  0.41073275, -0.31696847,  0.57553804,\n",
       "        1.8333681 ,  0.12940723,  1.2340578 , -1.6419202 , -0.9788594 ,\n",
       "        0.20720868,  0.32312712, -0.985142  , -0.22679777,  0.29951668,\n",
       "       -1.3301425 ,  0.8099032 , -0.03627906, -0.88904995,  0.13269156,\n",
       "       -0.7226151 , -0.75722003, -0.86820215,  0.12835236, -0.14158279,\n",
       "        0.03930757, -0.17220211, -0.55649775, -0.6135143 ,  1.0592129 ,\n",
       "       -0.31633103, -0.0503987 , -1.080164  , -0.8931076 ,  0.8697018 ,\n",
       "       -0.01947115,  0.83534986, -0.12876064,  0.82707775, -0.47081432,\n",
       "       -0.22527802,  0.7351233 ,  1.0684599 ,  0.06558256, -0.45236933,\n",
       "       -0.40911976, -1.065094  , -1.4948546 ,  0.11158447,  0.67132676,\n",
       "        0.46024045,  1.2923009 ,  0.03524973, -0.06889033, -1.2251862 ,\n",
       "       -0.36549658,  0.08726263, -0.54474634, -0.63210934, -0.49089146,\n",
       "       -0.06747297, -1.0530839 , -1.1813622 ,  0.08844626,  0.92408574,\n",
       "       -1.0275584 ,  0.20468706,  1.0644802 , -0.59908974,  0.30608392,\n",
       "        0.66199213, -0.24295801, -0.52946866,  0.42011288, -0.16854328,\n",
       "        0.8290444 , -0.37906334,  0.42296585, -0.34635127, -0.20098057,\n",
       "        0.8398056 , -0.1993033 , -1.3095819 ,  0.06879937, -0.7494756 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access to the embedings\n",
    "\n",
    "model.wv['the']  # Vector embeding of a word. Numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "Gd6-usJkHQOp",
    "outputId": "e58d1184-1684-44df-d126-a8cf0ded3261",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-29 10:05:27,144 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similars to woman: [('girl', 0.9787725210189819), ('boy', 0.9499495625495911), ('man', 0.939125120639801), ('child', 0.9363545775413513), ('young', 0.9359468817710876), ('distaste', 0.9319871068000793), ('artist', 0.9301795363426208), ('youngster', 0.9295510053634644), ('remark', 0.9289333820343018), ('fellow', 0.9283180832862854)] \n",
      "\n",
      "Indetify the word that doesn't match in a list: cereal \n",
      "\n",
      "Words similarity (woman - man): 0.87825197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "# Similarity fucntions\n",
    "\n",
    "print('Similars to woman:', model.wv.most_similar_cosmul(positive=['woman']), '\\n')\n",
    "\n",
    "print(\"Indetify the word that doesn't match in a list:\", model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split()), '\\n')\n",
    "\n",
    "print('Words similarity (woman - man):', model.wv.similarity('woman', 'man'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "aSm5yHDFHQOt",
    "outputId": "69158118-7268-48f4-ae99-a802c9d2fdb1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-29 10:05:27,952 : INFO : Evaluating word analogies for top 300000 words in the model on /usr/local/lib/python3.6/dist-packages/gensim/test/test_data/questions-words.txt\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "2019-10-29 10:05:28,040 : INFO : capital-common-countries: 2.2% (2/90)\n",
      "2019-10-29 10:05:28,106 : INFO : capital-world: 0.0% (0/44)\n",
      "2019-10-29 10:05:28,126 : INFO : currency: 0.0% (0/12)\n",
      "2019-10-29 10:05:28,521 : INFO : city-in-state: 0.4% (2/457)\n",
      "2019-10-29 10:05:28,702 : INFO : family: 26.7% (56/210)\n",
      "2019-10-29 10:05:29,357 : INFO : gram1-adjective-to-adverb: 0.7% (5/756)\n",
      "2019-10-29 10:05:29,471 : INFO : gram2-opposite: 0.0% (0/132)\n",
      "2019-10-29 10:05:30,307 : INFO : gram3-comparative: 6.2% (65/1056)\n",
      "2019-10-29 10:05:30,485 : INFO : gram4-superlative: 0.0% (0/210)\n",
      "2019-10-29 10:05:30,994 : INFO : gram5-present-participle: 2.5% (16/650)\n",
      "2019-10-29 10:05:31,241 : INFO : gram6-nationality-adjective: 0.3% (1/297)\n",
      "2019-10-29 10:05:32,221 : INFO : gram7-past-tense: 1.8% (23/1260)\n",
      "2019-10-29 10:05:32,655 : INFO : gram8-plural: 0.9% (5/552)\n",
      "2019-10-29 10:05:32,937 : INFO : gram9-plural-verbs: 1.8% (6/342)\n",
      "2019-10-29 10:05:32,947 : INFO : Quadruplets with out-of-vocabulary words: 69.0%\n",
      "2019-10-29 10:05:32,951 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2019-10-29 10:05:32,955 : INFO : Total accuracy: 3.0% (181/6068)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country - capital corrects [('BERLIN', 'GERMANY', 'MOSCOW', 'RUSSIA'), ('BERLIN', 'GERMANY', 'PARIS', 'FRANCE')]\n"
     ]
    }
   ],
   "source": [
    "# Check the accuracy of the builded embedings over a standar evaluation list of relations\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "analogy_scores = model.wv.evaluate_word_analogies(datapath('questions-words.txt'))\n",
    "\n",
    "print('country - capital corrects', analogy_scores[1][0]['correct'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "QLtMImeEKv27",
    "outputId": "fbeab9fc-011a-40bc-f913-db078728094d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BERLIN', 'GERMANY', 'MOSCOW', 'RUSSIA'),\n",
       " ('BERLIN', 'GERMANY', 'PARIS', 'FRANCE')]"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_scores[1][0]['correct']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "KeZldA6wHQOw",
    "outputId": "1de3e82a-76e3-45fb-d873-96ed5f65ea24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERLIN GERMANY PARIS FRANCE =  [('France', 0.9416736960411072), ('Sundays', 0.9277036786079407), ('London', 0.9190959930419922), ('King', 0.9176918268203735), ('editorials', 0.9164081811904907), ('laundering', 0.9160078763961792), ('late', 0.9101332426071167), ('Cologne', 0.907745897769928), ('Boston', 0.9075751304626465), ('harness', 0.9045456647872925)] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print('BERLIN GERMANY PARIS FRANCE = ', model.wv.most_similar(positive=['Germany', 'Paris'], negative=['Berlin']), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "vDQlJGjuHQOy",
    "outputId": "14a6e14c-7c2d-42f1-f848-56c5adf6ecd5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-29 10:05:33,033 : INFO : saving Word2VecKeyedVectors object under word_vectors.gz, separately None\n",
      "2019-10-29 10:05:33,035 : INFO : not storing attribute vectors_norm\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-10-29 10:05:34,986 : INFO : saved word_vectors.gz\n"
     ]
    }
   ],
   "source": [
    "# If you finish to train the model. Save only the embedings and delete model.\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors = model.wv\n",
    "\n",
    "fname = 'word_vectors.gz'\n",
    "word_vectors.save(fname)\n",
    "\n",
    "del model\n",
    "\n",
    "# To load:\n",
    "# word_vectors = KeyedVectors.load(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "U-J98Ad_HQO0",
    "outputId": "f78beb26-a8d8-47cb-c652-52474e11d302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of embedings: 15173 \n",
      "\n",
      "Sample of words available (20 first): ['The', 'Fulton', 'County', 'Grand', 'said', 'Friday', 'an', 'investigation', 'of', 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities'] \n",
      "\n",
      "Vocab word attributes for \"Oregon\" word: Vocab(count:11, index:7849, sample_int:4294967296) \n",
      "\n",
      "Word embedings for \"Oregon\" word: [ 0.01174455 -0.0525909   0.03817609  0.03665612  0.09587453 -0.08884571\n",
      " -0.00556803  0.03032871  0.05244231 -0.04206459  0.04749648 -0.04230019\n",
      " -0.00608735 -0.08207362  0.02439372 -0.05283214  0.08345121  0.12732227\n",
      " -0.05143046 -0.07548333  0.10583635  0.01633846  0.03692887 -0.14814882\n",
      " -0.06239773  0.04670925 -0.0554959  -0.06597858  0.16403872 -0.06255036\n",
      " -0.05700822  0.08580984 -0.01782249 -0.02151349  0.09711486  0.00588975\n",
      "  0.05912919  0.1457797  -0.10314389 -0.0413823   0.04486847 -0.16086109\n",
      " -0.00623296 -0.04546982  0.11388588 -0.03013373 -0.07861289  0.05028199\n",
      " -0.19711144  0.01380901 -0.08825806  0.02179422  0.03897243 -0.01637251\n",
      " -0.0075431  -0.00723948 -0.06209785  0.01012394 -0.04107125 -0.0419466\n",
      "  0.09150214  0.02610716  0.04255244  0.04496781 -0.0696028   0.01833444\n",
      "  0.0894555  -0.02633211  0.00897781 -0.01955874  0.06624299  0.08867738\n",
      " -0.00861505 -0.11894374 -0.00917944 -0.06135319  0.06232916 -0.03219327\n",
      " -0.01509426  0.08197818 -0.07330862  0.11048159 -0.0671156  -0.06650911\n",
      "  0.00035265 -0.01398013  0.053852    0.06033536 -0.01911265  0.01299119\n",
      " -0.0101566   0.0864513   0.01317578  0.05082809 -0.05604335  0.09091087\n",
      " -0.20039225 -0.0307932   0.02679589 -0.03259188]\n"
     ]
    }
   ],
   "source": [
    "# Explore the embedings\n",
    "\n",
    "print('Num of embedings:', len(word_vectors.vocab.keys()), '\\n')\n",
    "\n",
    "print('Sample of words available (20 first):', list(word_vectors.vocab.keys())[:20], '\\n')\n",
    "\n",
    "print('Vocab word attributes for \"Oregon\" word:', word_vectors.vocab['Oregon'], '\\n')\n",
    "\n",
    "print('Word embedings for \"Oregon\" word:', word_vectors['Oregon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0b8N44ctHQO2",
    "outputId": "f620312c-b5b0-4be7-b280-582b9acb85b7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 7258\n",
      "said 1943\n",
      "an 3542\n",
      "of 36080\n",
      "`` 8837\n",
      "no 1781\n",
      "'' 8789\n",
      "that 10237\n",
      "any 1301\n",
      ". 49346\n",
      "in 19536\n",
      "the 62713\n",
      ", 58334\n",
      "which 3540\n",
      "had 5102\n",
      "and 27915\n",
      "for 8841\n",
      "was 9777\n",
      "been 2470\n",
      "by 5103\n",
      "to 25732\n",
      "a 21881\n",
      "such 1192\n",
      "this 3966\n",
      "it 6723\n",
      "are 4333\n",
      "or 4118\n",
      "It 2037\n",
      "have 3892\n",
      "these 1228\n",
      "them 1786\n",
      "on 6395\n",
      "other 1627\n",
      "two 1311\n",
      "be 6344\n",
      "is 10011\n",
      "as 6706\n",
      "so 1755\n",
      "may 1292\n",
      "at 4963\n",
      "This 1179\n",
      "one 2873\n",
      "but 3007\n",
      "has 2425\n",
      "all 2726\n",
      "with 7012\n",
      "they 2773\n",
      "our 1142\n",
      "we 1973\n",
      "some 1407\n",
      "do 1259\n",
      "will 2204\n",
      "its 1780\n",
      "into 1782\n",
      "from 4207\n",
      "new 1060\n",
      "when 1746\n",
      "not 4423\n",
      "there 1877\n",
      ": 1795\n",
      "( 2435\n",
      ") 2466\n",
      "his 6466\n",
      "man 1151\n",
      "more 2130\n",
      "than 1788\n",
      "He 2982\n",
      "who 2192\n",
      "he 6566\n",
      "would 2677\n",
      "up 1874\n",
      "In 1801\n",
      "out 2058\n",
      "-- 3432\n",
      "were 3279\n",
      "first 1242\n",
      "must 1003\n",
      "time 1556\n",
      "what 1435\n",
      "made 1122\n",
      "then 1025\n",
      "most 1055\n",
      "A 1314\n",
      "about 1766\n",
      "can 1738\n",
      "like 1237\n",
      "But 1374\n",
      "him 2576\n",
      "I 5161\n",
      "over 1206\n",
      "now 1045\n",
      "their 2562\n",
      "you 2766\n",
      "only 1646\n",
      "if 1466\n",
      "my 1161\n",
      "could 1580\n",
      "? 4693\n",
      "; 5566\n",
      "me 1165\n",
      "! 1596\n",
      "she 1949\n",
      "her 2885\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary frequency. List the words with freq > 1000\n",
    "\n",
    "for k in word_vectors.vocab.keys():\n",
    "    if word_vectors.vocab[k].count > 1000:\n",
    "        print(k, word_vectors.vocab[k].count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wpQjv1TbHQO5"
   },
   "source": [
    "## Load pretrained vectors and use it\n",
    "\n",
    "Load pretrained vectors (300Mb) from\n",
    " http://nlpserver2.inf.ufrgs.br/alexandres/vectors/lexvec.enwiki%2bnewscrawl.300d.W.pos.vectors.gz \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "tWMXPFO-H9z_",
    "outputId": "d59362df-deec-4ab7-ec43-8062d76fcde3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-29 10:05:35--  https://www.dropbox.com/s/kguufyc2xcdi8yk/lexvec.enwiki%2Bnewscrawl.300d.W.pos.vectors.gz\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.1, 2620:100:6016:1::a27d:101\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/kguufyc2xcdi8yk/lexvec.enwiki%2Bnewscrawl.300d.W.pos.vectors.gz [following]\n",
      "--2019-10-29 10:05:36--  https://www.dropbox.com/s/raw/kguufyc2xcdi8yk/lexvec.enwiki%2Bnewscrawl.300d.W.pos.vectors.gz\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc2aec4abe5a308bc2e5de22a797.dl.dropboxusercontent.com/cd/0/inline/ArXt6Rz5nGvKVlOBdSDJN_ZRg1m22DJwjSaBx98hQkL386kLeKBIGR77oopWqvskYPXtAhtBkyaPbKFssM7EL6aLuMgvVAZ1shP3yni5WLCCjQ/file# [following]\n",
      "--2019-10-29 10:05:36--  https://uc2aec4abe5a308bc2e5de22a797.dl.dropboxusercontent.com/cd/0/inline/ArXt6Rz5nGvKVlOBdSDJN_ZRg1m22DJwjSaBx98hQkL386kLeKBIGR77oopWqvskYPXtAhtBkyaPbKFssM7EL6aLuMgvVAZ1shP3yni5WLCCjQ/file\n",
      "Resolving uc2aec4abe5a308bc2e5de22a797.dl.dropboxusercontent.com (uc2aec4abe5a308bc2e5de22a797.dl.dropboxusercontent.com)... 162.125.1.6, 2620:100:6016:6::a27d:106\n",
      "Connecting to uc2aec4abe5a308bc2e5de22a797.dl.dropboxusercontent.com (uc2aec4abe5a308bc2e5de22a797.dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: /cd/0/inline2/ArXBZrBZj1Hr1XmC4CyxWvN23OXaCs4b3RbyUrePrXCGyj1Ttih3LuPhM1YcXfPQ_ZO-MtF1rGi__mEJOdUpFLWLNHN23IXjjT8NmfbPZ0AEsHQWCF46PYVf7G4bPMd2aDU4jh5AaZd4vqrEB2-yfB806a1LW1RQfGE2RGsokN1mCqHaGI6AU4Sj6Ym0PSzdWhSuyU3-7zvnoXRjujzSxNKH536Z1Ar8c9YcjZ_sLn6f1v4XnFM3zUT7xN9DU57EJ93oUxOD06uGBkD3QynsHoVlwocUYNbs3Fo7i9ZdIfF-UyVpMMQtBtjNhQCjyfkvA4Ao6dQZrjZBA3fw9WcqIF42/file [following]\n",
      "--2019-10-29 10:05:36--  https://uc2aec4abe5a308bc2e5de22a797.dl.dropboxusercontent.com/cd/0/inline2/ArXBZrBZj1Hr1XmC4CyxWvN23OXaCs4b3RbyUrePrXCGyj1Ttih3LuPhM1YcXfPQ_ZO-MtF1rGi__mEJOdUpFLWLNHN23IXjjT8NmfbPZ0AEsHQWCF46PYVf7G4bPMd2aDU4jh5AaZd4vqrEB2-yfB806a1LW1RQfGE2RGsokN1mCqHaGI6AU4Sj6Ym0PSzdWhSuyU3-7zvnoXRjujzSxNKH536Z1Ar8c9YcjZ_sLn6f1v4XnFM3zUT7xN9DU57EJ93oUxOD06uGBkD3QynsHoVlwocUYNbs3Fo7i9ZdIfF-UyVpMMQtBtjNhQCjyfkvA4Ao6dQZrjZBA3fw9WcqIF42/file\n",
      "Reusing existing connection to uc2aec4abe5a308bc2e5de22a797.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 417422003 (398M) [application/octet-stream]\n",
      "Saving to: ‘lexvec.enwiki+newscrawl.300d.W.pos.vectors.gz’\n",
      "\n",
      "lexvec.enwiki+newsc 100%[===================>] 398.08M  36.0MB/s    in 11s     \n",
      "\n",
      "2019-10-29 10:05:48 (37.1 MB/s) - ‘lexvec.enwiki+newscrawl.300d.W.pos.vectors.gz’ saved [417422003/417422003]\n",
      "\n",
      "gzip: lexvec.enwiki+newscrawl.300d.W.pos.vectors already exists; do you wish to overwrite (y or n)? y\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "# Download vectors into colab\n",
    "! wget https://www.dropbox.com/s/kguufyc2xcdi8yk/lexvec.enwiki%2Bnewscrawl.300d.W.pos.vectors.gz\n",
    "\n",
    "! gunzip lexvec.enwiki+newscrawl.300d.W.pos.vectors.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "qGhxyB4-HQO6",
    "outputId": "c644034e-d722-4367-e308-30ceac67147b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-29 10:24:27,866 : INFO : loading projection weights from ./lexvec.enwiki+newscrawl.300d.W.pos.vectors\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-10-29 10:26:10,124 : INFO : loaded (368999, 300) matrix from ./lexvec.enwiki+newscrawl.300d.W.pos.vectors\n",
      "2019-10-29 10:26:10,144 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of one embeding\n",
      "Shape of one embeding: (300,)\n",
      "First 10 embedings of \"dog\": [ 0.037035 -0.062638  0.11338   0.088951  0.075585  0.049152  0.129707\n",
      " -0.04612  -0.167092 -0.186152] \n",
      "\n",
      "woman + king - man =  [('queen', 0.6212160587310791), ('monarch', 0.5939740538597107), ('prince', 0.5655953884124756), ('throne', 0.5191947817802429), ('princess', 0.5010462403297424), ('emperor', 0.494123637676239), ('consort', 0.4744756519794464), ('empress', 0.4712255001068115), ('regent', 0.46504777669906616), ('betrothed', 0.4598500430583954)] \n",
      "\n",
      "Doesn't match: cereal \n",
      "\n",
      "Similarity woman - man: 0.8137212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained embedings\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(os.path.join(data_path, 'lexvec.enwiki+newscrawl.300d.W.pos.vectors'),\n",
    "                                          unicode_errors='ignore')\n",
    "\n",
    "print('Sample of one embeding')\n",
    "dog = model['dog']\n",
    "print('Shape of one embeding:', dog.shape)\n",
    "print('First 10 embedings of \"dog\":', dog[:10], '\\n')\n",
    "\n",
    "\n",
    "# Some predefined functions that show content related information for given words\n",
    "print('woman + king - man = ', model.most_similar(positive=['woman', 'king'], negative=['man']), '\\n')\n",
    "\n",
    "print(\"Doesn't match:\", model.doesnt_match(\"breakfast cereal dinner lunch\".split()), '\\n')\n",
    "\n",
    "print('Similarity woman - man:', model.similarity('woman', 'man'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['good'], negative=['bad'], topn=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['bad'], negative=['good'], topn=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "OO5hwBPaHQO8",
    "outputId": "e7fbc9dd-6077-4f1f-bbe4-752cba8bd8f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2019-10-29 10:26:12,566 : INFO : Evaluating word analogies for top 300000 words in the model on /usr/local/lib/python3.6/dist-packages/gensim/test/test_data/questions-words.txt\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "2019-10-29 10:26:39,880 : INFO : capital-common-countries: 95.1% (481/506)\n",
      "2019-10-29 10:30:44,547 : INFO : capital-world: 94.4% (4269/4524)\n",
      "2019-10-29 10:31:31,482 : INFO : currency: 22.1% (191/866)\n",
      "2019-10-29 10:33:45,000 : INFO : city-in-state: 72.6% (1792/2467)\n",
      "2019-10-29 10:34:12,246 : INFO : family: 87.7% (444/506)\n",
      "2019-10-29 10:35:06,321 : INFO : gram1-adjective-to-adverb: 24.9% (247/992)\n",
      "2019-10-29 10:35:49,837 : INFO : gram2-opposite: 36.6% (297/812)\n",
      "2019-10-29 10:37:01,387 : INFO : gram3-comparative: 87.3% (1163/1332)\n",
      "2019-10-29 10:38:01,093 : INFO : gram4-superlative: 66.8% (749/1122)\n",
      "2019-10-29 10:38:40,003 : INFO : gram5-present-participle: 62.7% (662/1056)\n",
      "2019-10-29 10:39:38,579 : INFO : gram6-nationality-adjective: 91.8% (1468/1599)\n",
      "2019-10-29 10:40:37,401 : INFO : gram7-past-tense: 60.0% (936/1560)\n",
      "2019-10-29 10:41:28,250 : INFO : gram8-plural: 76.2% (1015/1332)\n",
      "2019-10-29 10:42:00,848 : INFO : gram9-plural-verbs: 59.8% (520/870)\n",
      "2019-10-29 10:42:00,854 : INFO : Quadruplets with out-of-vocabulary words: 0.0%\n",
      "2019-10-29 10:42:00,857 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2019-10-29 10:42:00,860 : INFO : Total accuracy: 72.8% (14234/19544)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country - capital corrects [('ATHENS', 'GREECE', 'BAGHDAD', 'IRAQ'), ('ATHENS', 'GREECE', 'BANGKOK', 'THAILAND'), ('ATHENS', 'GREECE', 'BEIJING', 'CHINA'), ('ATHENS', 'GREECE', 'BERLIN', 'GERMANY'), ('ATHENS', 'GREECE', 'BERN', 'SWITZERLAND'), ('ATHENS', 'GREECE', 'CAIRO', 'EGYPT'), ('ATHENS', 'GREECE', 'HANOI', 'VIETNAM'), ('ATHENS', 'GREECE', 'HAVANA', 'CUBA'), ('ATHENS', 'GREECE', 'HELSINKI', 'FINLAND'), ('ATHENS', 'GREECE', 'ISLAMABAD', 'PAKISTAN'), ('ATHENS', 'GREECE', 'KABUL', 'AFGHANISTAN'), ('ATHENS', 'GREECE', 'LONDON', 'ENGLAND'), ('ATHENS', 'GREECE', 'MADRID', 'SPAIN'), ('ATHENS', 'GREECE', 'MOSCOW', 'RUSSIA'), ('ATHENS', 'GREECE', 'OSLO', 'NORWAY'), ('ATHENS', 'GREECE', 'PARIS', 'FRANCE'), ('ATHENS', 'GREECE', 'ROME', 'ITALY'), ('ATHENS', 'GREECE', 'STOCKHOLM', 'SWEDEN'), ('ATHENS', 'GREECE', 'TEHRAN', 'IRAN'), ('ATHENS', 'GREECE', 'TOKYO', 'JAPAN'), ('BAGHDAD', 'IRAQ', 'BANGKOK', 'THAILAND'), ('BAGHDAD', 'IRAQ', 'BEIJING', 'CHINA'), ('BAGHDAD', 'IRAQ', 'BERLIN', 'GERMANY'), ('BAGHDAD', 'IRAQ', 'BERN', 'SWITZERLAND'), ('BAGHDAD', 'IRAQ', 'CAIRO', 'EGYPT'), ('BAGHDAD', 'IRAQ', 'HANOI', 'VIETNAM'), ('BAGHDAD', 'IRAQ', 'HAVANA', 'CUBA'), ('BAGHDAD', 'IRAQ', 'HELSINKI', 'FINLAND'), ('BAGHDAD', 'IRAQ', 'ISLAMABAD', 'PAKISTAN'), ('BAGHDAD', 'IRAQ', 'KABUL', 'AFGHANISTAN'), ('BAGHDAD', 'IRAQ', 'MADRID', 'SPAIN'), ('BAGHDAD', 'IRAQ', 'MOSCOW', 'RUSSIA'), ('BAGHDAD', 'IRAQ', 'OSLO', 'NORWAY'), ('BAGHDAD', 'IRAQ', 'OTTAWA', 'CANADA'), ('BAGHDAD', 'IRAQ', 'PARIS', 'FRANCE'), ('BAGHDAD', 'IRAQ', 'ROME', 'ITALY'), ('BAGHDAD', 'IRAQ', 'STOCKHOLM', 'SWEDEN'), ('BAGHDAD', 'IRAQ', 'TEHRAN', 'IRAN'), ('BAGHDAD', 'IRAQ', 'TOKYO', 'JAPAN'), ('BAGHDAD', 'IRAQ', 'ATHENS', 'GREECE'), ('BANGKOK', 'THAILAND', 'BEIJING', 'CHINA'), ('BANGKOK', 'THAILAND', 'BERLIN', 'GERMANY'), ('BANGKOK', 'THAILAND', 'BERN', 'SWITZERLAND'), ('BANGKOK', 'THAILAND', 'CAIRO', 'EGYPT'), ('BANGKOK', 'THAILAND', 'CANBERRA', 'AUSTRALIA'), ('BANGKOK', 'THAILAND', 'HANOI', 'VIETNAM'), ('BANGKOK', 'THAILAND', 'HAVANA', 'CUBA'), ('BANGKOK', 'THAILAND', 'HELSINKI', 'FINLAND'), ('BANGKOK', 'THAILAND', 'ISLAMABAD', 'PAKISTAN'), ('BANGKOK', 'THAILAND', 'KABUL', 'AFGHANISTAN'), ('BANGKOK', 'THAILAND', 'LONDON', 'ENGLAND'), ('BANGKOK', 'THAILAND', 'MADRID', 'SPAIN'), ('BANGKOK', 'THAILAND', 'MOSCOW', 'RUSSIA'), ('BANGKOK', 'THAILAND', 'OSLO', 'NORWAY'), ('BANGKOK', 'THAILAND', 'OTTAWA', 'CANADA'), ('BANGKOK', 'THAILAND', 'PARIS', 'FRANCE'), ('BANGKOK', 'THAILAND', 'ROME', 'ITALY'), ('BANGKOK', 'THAILAND', 'STOCKHOLM', 'SWEDEN'), ('BANGKOK', 'THAILAND', 'TEHRAN', 'IRAN'), ('BANGKOK', 'THAILAND', 'TOKYO', 'JAPAN'), ('BANGKOK', 'THAILAND', 'ATHENS', 'GREECE'), ('BANGKOK', 'THAILAND', 'BAGHDAD', 'IRAQ'), ('BEIJING', 'CHINA', 'BERLIN', 'GERMANY'), ('BEIJING', 'CHINA', 'CAIRO', 'EGYPT'), ('BEIJING', 'CHINA', 'CANBERRA', 'AUSTRALIA'), ('BEIJING', 'CHINA', 'HANOI', 'VIETNAM'), ('BEIJING', 'CHINA', 'HAVANA', 'CUBA'), ('BEIJING', 'CHINA', 'HELSINKI', 'FINLAND'), ('BEIJING', 'CHINA', 'ISLAMABAD', 'PAKISTAN'), ('BEIJING', 'CHINA', 'KABUL', 'AFGHANISTAN'), ('BEIJING', 'CHINA', 'MADRID', 'SPAIN'), ('BEIJING', 'CHINA', 'MOSCOW', 'RUSSIA'), ('BEIJING', 'CHINA', 'OSLO', 'NORWAY'), ('BEIJING', 'CHINA', 'OTTAWA', 'CANADA'), ('BEIJING', 'CHINA', 'PARIS', 'FRANCE'), ('BEIJING', 'CHINA', 'ROME', 'ITALY'), ('BEIJING', 'CHINA', 'STOCKHOLM', 'SWEDEN'), ('BEIJING', 'CHINA', 'TEHRAN', 'IRAN'), ('BEIJING', 'CHINA', 'TOKYO', 'JAPAN'), ('BEIJING', 'CHINA', 'ATHENS', 'GREECE'), ('BEIJING', 'CHINA', 'BAGHDAD', 'IRAQ'), ('BEIJING', 'CHINA', 'BANGKOK', 'THAILAND'), ('BERLIN', 'GERMANY', 'BERN', 'SWITZERLAND'), ('BERLIN', 'GERMANY', 'CAIRO', 'EGYPT'), ('BERLIN', 'GERMANY', 'CANBERRA', 'AUSTRALIA'), ('BERLIN', 'GERMANY', 'HANOI', 'VIETNAM'), ('BERLIN', 'GERMANY', 'HAVANA', 'CUBA'), ('BERLIN', 'GERMANY', 'HELSINKI', 'FINLAND'), ('BERLIN', 'GERMANY', 'ISLAMABAD', 'PAKISTAN'), ('BERLIN', 'GERMANY', 'KABUL', 'AFGHANISTAN'), ('BERLIN', 'GERMANY', 'LONDON', 'ENGLAND'), ('BERLIN', 'GERMANY', 'MADRID', 'SPAIN'), ('BERLIN', 'GERMANY', 'MOSCOW', 'RUSSIA'), ('BERLIN', 'GERMANY', 'OSLO', 'NORWAY'), ('BERLIN', 'GERMANY', 'OTTAWA', 'CANADA'), ('BERLIN', 'GERMANY', 'PARIS', 'FRANCE'), ('BERLIN', 'GERMANY', 'ROME', 'ITALY'), ('BERLIN', 'GERMANY', 'STOCKHOLM', 'SWEDEN'), ('BERLIN', 'GERMANY', 'TEHRAN', 'IRAN'), ('BERLIN', 'GERMANY', 'TOKYO', 'JAPAN'), ('BERLIN', 'GERMANY', 'ATHENS', 'GREECE'), ('BERLIN', 'GERMANY', 'BAGHDAD', 'IRAQ'), ('BERLIN', 'GERMANY', 'BANGKOK', 'THAILAND'), ('BERLIN', 'GERMANY', 'BEIJING', 'CHINA'), ('BERN', 'SWITZERLAND', 'CAIRO', 'EGYPT'), ('BERN', 'SWITZERLAND', 'CANBERRA', 'AUSTRALIA'), ('BERN', 'SWITZERLAND', 'HANOI', 'VIETNAM'), ('BERN', 'SWITZERLAND', 'HAVANA', 'CUBA'), ('BERN', 'SWITZERLAND', 'HELSINKI', 'FINLAND'), ('BERN', 'SWITZERLAND', 'ISLAMABAD', 'PAKISTAN'), ('BERN', 'SWITZERLAND', 'KABUL', 'AFGHANISTAN'), ('BERN', 'SWITZERLAND', 'MADRID', 'SPAIN'), ('BERN', 'SWITZERLAND', 'MOSCOW', 'RUSSIA'), ('BERN', 'SWITZERLAND', 'OSLO', 'NORWAY'), ('BERN', 'SWITZERLAND', 'OTTAWA', 'CANADA'), ('BERN', 'SWITZERLAND', 'PARIS', 'FRANCE'), ('BERN', 'SWITZERLAND', 'ROME', 'ITALY'), ('BERN', 'SWITZERLAND', 'STOCKHOLM', 'SWEDEN'), ('BERN', 'SWITZERLAND', 'TEHRAN', 'IRAN'), ('BERN', 'SWITZERLAND', 'TOKYO', 'JAPAN'), ('BERN', 'SWITZERLAND', 'ATHENS', 'GREECE'), ('BERN', 'SWITZERLAND', 'BAGHDAD', 'IRAQ'), ('BERN', 'SWITZERLAND', 'BANGKOK', 'THAILAND'), ('BERN', 'SWITZERLAND', 'BEIJING', 'CHINA'), ('BERN', 'SWITZERLAND', 'BERLIN', 'GERMANY'), ('CAIRO', 'EGYPT', 'CANBERRA', 'AUSTRALIA'), ('CAIRO', 'EGYPT', 'HANOI', 'VIETNAM'), ('CAIRO', 'EGYPT', 'HAVANA', 'CUBA'), ('CAIRO', 'EGYPT', 'HELSINKI', 'FINLAND'), ('CAIRO', 'EGYPT', 'ISLAMABAD', 'PAKISTAN'), ('CAIRO', 'EGYPT', 'KABUL', 'AFGHANISTAN'), ('CAIRO', 'EGYPT', 'MADRID', 'SPAIN'), ('CAIRO', 'EGYPT', 'MOSCOW', 'RUSSIA'), ('CAIRO', 'EGYPT', 'OSLO', 'NORWAY'), ('CAIRO', 'EGYPT', 'OTTAWA', 'CANADA'), ('CAIRO', 'EGYPT', 'PARIS', 'FRANCE'), ('CAIRO', 'EGYPT', 'ROME', 'ITALY'), ('CAIRO', 'EGYPT', 'STOCKHOLM', 'SWEDEN'), ('CAIRO', 'EGYPT', 'TEHRAN', 'IRAN'), ('CAIRO', 'EGYPT', 'TOKYO', 'JAPAN'), ('CAIRO', 'EGYPT', 'ATHENS', 'GREECE'), ('CAIRO', 'EGYPT', 'BAGHDAD', 'IRAQ'), ('CAIRO', 'EGYPT', 'BANGKOK', 'THAILAND'), ('CAIRO', 'EGYPT', 'BEIJING', 'CHINA'), ('CAIRO', 'EGYPT', 'BERLIN', 'GERMANY'), ('CANBERRA', 'AUSTRALIA', 'HANOI', 'VIETNAM'), ('CANBERRA', 'AUSTRALIA', 'HAVANA', 'CUBA'), ('CANBERRA', 'AUSTRALIA', 'HELSINKI', 'FINLAND'), ('CANBERRA', 'AUSTRALIA', 'ISLAMABAD', 'PAKISTAN'), ('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'), ('CANBERRA', 'AUSTRALIA', 'LONDON', 'ENGLAND'), ('CANBERRA', 'AUSTRALIA', 'MADRID', 'SPAIN'), ('CANBERRA', 'AUSTRALIA', 'MOSCOW', 'RUSSIA'), ('CANBERRA', 'AUSTRALIA', 'OSLO', 'NORWAY'), ('CANBERRA', 'AUSTRALIA', 'OTTAWA', 'CANADA'), ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'), ('CANBERRA', 'AUSTRALIA', 'ROME', 'ITALY'), ('CANBERRA', 'AUSTRALIA', 'STOCKHOLM', 'SWEDEN'), ('CANBERRA', 'AUSTRALIA', 'TEHRAN', 'IRAN'), ('CANBERRA', 'AUSTRALIA', 'TOKYO', 'JAPAN'), ('CANBERRA', 'AUSTRALIA', 'ATHENS', 'GREECE'), ('CANBERRA', 'AUSTRALIA', 'BAGHDAD', 'IRAQ'), ('CANBERRA', 'AUSTRALIA', 'BANGKOK', 'THAILAND'), ('CANBERRA', 'AUSTRALIA', 'BEIJING', 'CHINA'), ('CANBERRA', 'AUSTRALIA', 'BERLIN', 'GERMANY'), ('CANBERRA', 'AUSTRALIA', 'BERN', 'SWITZERLAND'), ('CANBERRA', 'AUSTRALIA', 'CAIRO', 'EGYPT'), ('HANOI', 'VIETNAM', 'HAVANA', 'CUBA'), ('HANOI', 'VIETNAM', 'HELSINKI', 'FINLAND'), ('HANOI', 'VIETNAM', 'ISLAMABAD', 'PAKISTAN'), ('HANOI', 'VIETNAM', 'KABUL', 'AFGHANISTAN'), ('HANOI', 'VIETNAM', 'MADRID', 'SPAIN'), ('HANOI', 'VIETNAM', 'MOSCOW', 'RUSSIA'), ('HANOI', 'VIETNAM', 'OSLO', 'NORWAY'), ('HANOI', 'VIETNAM', 'OTTAWA', 'CANADA'), ('HANOI', 'VIETNAM', 'PARIS', 'FRANCE'), ('HANOI', 'VIETNAM', 'ROME', 'ITALY'), ('HANOI', 'VIETNAM', 'STOCKHOLM', 'SWEDEN'), ('HANOI', 'VIETNAM', 'TEHRAN', 'IRAN'), ('HANOI', 'VIETNAM', 'TOKYO', 'JAPAN'), ('HANOI', 'VIETNAM', 'ATHENS', 'GREECE'), ('HANOI', 'VIETNAM', 'BAGHDAD', 'IRAQ'), ('HANOI', 'VIETNAM', 'BANGKOK', 'THAILAND'), ('HANOI', 'VIETNAM', 'BEIJING', 'CHINA'), ('HANOI', 'VIETNAM', 'BERLIN', 'GERMANY'), ('HANOI', 'VIETNAM', 'CAIRO', 'EGYPT'), ('HANOI', 'VIETNAM', 'CANBERRA', 'AUSTRALIA'), ('HAVANA', 'CUBA', 'HELSINKI', 'FINLAND'), ('HAVANA', 'CUBA', 'ISLAMABAD', 'PAKISTAN'), ('HAVANA', 'CUBA', 'KABUL', 'AFGHANISTAN'), ('HAVANA', 'CUBA', 'MADRID', 'SPAIN'), ('HAVANA', 'CUBA', 'MOSCOW', 'RUSSIA'), ('HAVANA', 'CUBA', 'OSLO', 'NORWAY'), ('HAVANA', 'CUBA', 'OTTAWA', 'CANADA'), ('HAVANA', 'CUBA', 'PARIS', 'FRANCE'), ('HAVANA', 'CUBA', 'ROME', 'ITALY'), ('HAVANA', 'CUBA', 'STOCKHOLM', 'SWEDEN'), ('HAVANA', 'CUBA', 'TEHRAN', 'IRAN'), ('HAVANA', 'CUBA', 'TOKYO', 'JAPAN'), ('HAVANA', 'CUBA', 'ATHENS', 'GREECE'), ('HAVANA', 'CUBA', 'BAGHDAD', 'IRAQ'), ('HAVANA', 'CUBA', 'BANGKOK', 'THAILAND'), ('HAVANA', 'CUBA', 'BEIJING', 'CHINA'), ('HAVANA', 'CUBA', 'BERLIN', 'GERMANY'), ('HAVANA', 'CUBA', 'BERN', 'SWITZERLAND'), ('HAVANA', 'CUBA', 'CAIRO', 'EGYPT'), ('HAVANA', 'CUBA', 'CANBERRA', 'AUSTRALIA'), ('HAVANA', 'CUBA', 'HANOI', 'VIETNAM'), ('HELSINKI', 'FINLAND', 'ISLAMABAD', 'PAKISTAN'), ('HELSINKI', 'FINLAND', 'KABUL', 'AFGHANISTAN'), ('HELSINKI', 'FINLAND', 'LONDON', 'ENGLAND'), ('HELSINKI', 'FINLAND', 'MADRID', 'SPAIN'), ('HELSINKI', 'FINLAND', 'MOSCOW', 'RUSSIA'), ('HELSINKI', 'FINLAND', 'OSLO', 'NORWAY'), ('HELSINKI', 'FINLAND', 'OTTAWA', 'CANADA'), ('HELSINKI', 'FINLAND', 'PARIS', 'FRANCE'), ('HELSINKI', 'FINLAND', 'ROME', 'ITALY'), ('HELSINKI', 'FINLAND', 'STOCKHOLM', 'SWEDEN'), ('HELSINKI', 'FINLAND', 'TEHRAN', 'IRAN'), ('HELSINKI', 'FINLAND', 'TOKYO', 'JAPAN'), ('HELSINKI', 'FINLAND', 'ATHENS', 'GREECE'), ('HELSINKI', 'FINLAND', 'BAGHDAD', 'IRAQ'), ('HELSINKI', 'FINLAND', 'BANGKOK', 'THAILAND'), ('HELSINKI', 'FINLAND', 'BEIJING', 'CHINA'), ('HELSINKI', 'FINLAND', 'BERLIN', 'GERMANY'), ('HELSINKI', 'FINLAND', 'BERN', 'SWITZERLAND'), ('HELSINKI', 'FINLAND', 'CAIRO', 'EGYPT'), ('HELSINKI', 'FINLAND', 'CANBERRA', 'AUSTRALIA'), ('HELSINKI', 'FINLAND', 'HANOI', 'VIETNAM'), ('HELSINKI', 'FINLAND', 'HAVANA', 'CUBA'), ('ISLAMABAD', 'PAKISTAN', 'KABUL', 'AFGHANISTAN'), ('ISLAMABAD', 'PAKISTAN', 'LONDON', 'ENGLAND'), ('ISLAMABAD', 'PAKISTAN', 'MADRID', 'SPAIN'), ('ISLAMABAD', 'PAKISTAN', 'MOSCOW', 'RUSSIA'), ('ISLAMABAD', 'PAKISTAN', 'OSLO', 'NORWAY'), ('ISLAMABAD', 'PAKISTAN', 'OTTAWA', 'CANADA'), ('ISLAMABAD', 'PAKISTAN', 'PARIS', 'FRANCE'), ('ISLAMABAD', 'PAKISTAN', 'ROME', 'ITALY'), ('ISLAMABAD', 'PAKISTAN', 'STOCKHOLM', 'SWEDEN'), ('ISLAMABAD', 'PAKISTAN', 'TEHRAN', 'IRAN'), ('ISLAMABAD', 'PAKISTAN', 'TOKYO', 'JAPAN'), ('ISLAMABAD', 'PAKISTAN', 'ATHENS', 'GREECE'), ('ISLAMABAD', 'PAKISTAN', 'BAGHDAD', 'IRAQ'), ('ISLAMABAD', 'PAKISTAN', 'BANGKOK', 'THAILAND'), ('ISLAMABAD', 'PAKISTAN', 'BEIJING', 'CHINA'), ('ISLAMABAD', 'PAKISTAN', 'BERLIN', 'GERMANY'), ('ISLAMABAD', 'PAKISTAN', 'BERN', 'SWITZERLAND'), ('ISLAMABAD', 'PAKISTAN', 'CAIRO', 'EGYPT'), ('ISLAMABAD', 'PAKISTAN', 'CANBERRA', 'AUSTRALIA'), ('ISLAMABAD', 'PAKISTAN', 'HANOI', 'VIETNAM'), ('ISLAMABAD', 'PAKISTAN', 'HAVANA', 'CUBA'), ('ISLAMABAD', 'PAKISTAN', 'HELSINKI', 'FINLAND'), ('KABUL', 'AFGHANISTAN', 'MADRID', 'SPAIN'), ('KABUL', 'AFGHANISTAN', 'MOSCOW', 'RUSSIA'), ('KABUL', 'AFGHANISTAN', 'OSLO', 'NORWAY'), ('KABUL', 'AFGHANISTAN', 'OTTAWA', 'CANADA'), ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'), ('KABUL', 'AFGHANISTAN', 'ROME', 'ITALY'), ('KABUL', 'AFGHANISTAN', 'STOCKHOLM', 'SWEDEN'), ('KABUL', 'AFGHANISTAN', 'TEHRAN', 'IRAN'), ('KABUL', 'AFGHANISTAN', 'TOKYO', 'JAPAN'), ('KABUL', 'AFGHANISTAN', 'ATHENS', 'GREECE'), ('KABUL', 'AFGHANISTAN', 'BAGHDAD', 'IRAQ'), ('KABUL', 'AFGHANISTAN', 'BANGKOK', 'THAILAND'), ('KABUL', 'AFGHANISTAN', 'BEIJING', 'CHINA'), ('KABUL', 'AFGHANISTAN', 'BERLIN', 'GERMANY'), ('KABUL', 'AFGHANISTAN', 'CAIRO', 'EGYPT'), ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'), ('KABUL', 'AFGHANISTAN', 'HANOI', 'VIETNAM'), ('KABUL', 'AFGHANISTAN', 'HAVANA', 'CUBA'), ('KABUL', 'AFGHANISTAN', 'HELSINKI', 'FINLAND'), ('KABUL', 'AFGHANISTAN', 'ISLAMABAD', 'PAKISTAN'), ('LONDON', 'ENGLAND', 'MADRID', 'SPAIN'), ('LONDON', 'ENGLAND', 'MOSCOW', 'RUSSIA'), ('LONDON', 'ENGLAND', 'OSLO', 'NORWAY'), ('LONDON', 'ENGLAND', 'PARIS', 'FRANCE'), ('LONDON', 'ENGLAND', 'ROME', 'ITALY'), ('LONDON', 'ENGLAND', 'STOCKHOLM', 'SWEDEN'), ('LONDON', 'ENGLAND', 'TEHRAN', 'IRAN'), ('LONDON', 'ENGLAND', 'TOKYO', 'JAPAN'), ('LONDON', 'ENGLAND', 'ATHENS', 'GREECE'), ('LONDON', 'ENGLAND', 'BAGHDAD', 'IRAQ'), ('LONDON', 'ENGLAND', 'BANGKOK', 'THAILAND'), ('LONDON', 'ENGLAND', 'BEIJING', 'CHINA'), ('LONDON', 'ENGLAND', 'BERLIN', 'GERMANY'), ('LONDON', 'ENGLAND', 'CAIRO', 'EGYPT'), ('LONDON', 'ENGLAND', 'CANBERRA', 'AUSTRALIA'), ('LONDON', 'ENGLAND', 'HANOI', 'VIETNAM'), ('LONDON', 'ENGLAND', 'HAVANA', 'CUBA'), ('LONDON', 'ENGLAND', 'HELSINKI', 'FINLAND'), ('LONDON', 'ENGLAND', 'ISLAMABAD', 'PAKISTAN'), ('LONDON', 'ENGLAND', 'KABUL', 'AFGHANISTAN'), ('MADRID', 'SPAIN', 'MOSCOW', 'RUSSIA'), ('MADRID', 'SPAIN', 'OSLO', 'NORWAY'), ('MADRID', 'SPAIN', 'OTTAWA', 'CANADA'), ('MADRID', 'SPAIN', 'PARIS', 'FRANCE'), ('MADRID', 'SPAIN', 'ROME', 'ITALY'), ('MADRID', 'SPAIN', 'STOCKHOLM', 'SWEDEN'), ('MADRID', 'SPAIN', 'TEHRAN', 'IRAN'), ('MADRID', 'SPAIN', 'TOKYO', 'JAPAN'), ('MADRID', 'SPAIN', 'ATHENS', 'GREECE'), ('MADRID', 'SPAIN', 'BAGHDAD', 'IRAQ'), ('MADRID', 'SPAIN', 'BANGKOK', 'THAILAND'), ('MADRID', 'SPAIN', 'BEIJING', 'CHINA'), ('MADRID', 'SPAIN', 'BERLIN', 'GERMANY'), ('MADRID', 'SPAIN', 'BERN', 'SWITZERLAND'), ('MADRID', 'SPAIN', 'CAIRO', 'EGYPT'), ('MADRID', 'SPAIN', 'CANBERRA', 'AUSTRALIA'), ('MADRID', 'SPAIN', 'HANOI', 'VIETNAM'), ('MADRID', 'SPAIN', 'HAVANA', 'CUBA'), ('MADRID', 'SPAIN', 'HELSINKI', 'FINLAND'), ('MADRID', 'SPAIN', 'ISLAMABAD', 'PAKISTAN'), ('MADRID', 'SPAIN', 'KABUL', 'AFGHANISTAN'), ('MOSCOW', 'RUSSIA', 'OSLO', 'NORWAY'), ('MOSCOW', 'RUSSIA', 'OTTAWA', 'CANADA'), ('MOSCOW', 'RUSSIA', 'PARIS', 'FRANCE'), ('MOSCOW', 'RUSSIA', 'ROME', 'ITALY'), ('MOSCOW', 'RUSSIA', 'STOCKHOLM', 'SWEDEN'), ('MOSCOW', 'RUSSIA', 'TEHRAN', 'IRAN'), ('MOSCOW', 'RUSSIA', 'TOKYO', 'JAPAN'), ('MOSCOW', 'RUSSIA', 'ATHENS', 'GREECE'), ('MOSCOW', 'RUSSIA', 'BAGHDAD', 'IRAQ'), ('MOSCOW', 'RUSSIA', 'BANGKOK', 'THAILAND'), ('MOSCOW', 'RUSSIA', 'BEIJING', 'CHINA'), ('MOSCOW', 'RUSSIA', 'BERLIN', 'GERMANY'), ('MOSCOW', 'RUSSIA', 'BERN', 'SWITZERLAND'), ('MOSCOW', 'RUSSIA', 'CAIRO', 'EGYPT'), ('MOSCOW', 'RUSSIA', 'CANBERRA', 'AUSTRALIA'), ('MOSCOW', 'RUSSIA', 'HANOI', 'VIETNAM'), ('MOSCOW', 'RUSSIA', 'HAVANA', 'CUBA'), ('MOSCOW', 'RUSSIA', 'HELSINKI', 'FINLAND'), ('MOSCOW', 'RUSSIA', 'ISLAMABAD', 'PAKISTAN'), ('MOSCOW', 'RUSSIA', 'KABUL', 'AFGHANISTAN'), ('MOSCOW', 'RUSSIA', 'MADRID', 'SPAIN'), ('OSLO', 'NORWAY', 'OTTAWA', 'CANADA'), ('OSLO', 'NORWAY', 'PARIS', 'FRANCE'), ('OSLO', 'NORWAY', 'ROME', 'ITALY'), ('OSLO', 'NORWAY', 'STOCKHOLM', 'SWEDEN'), ('OSLO', 'NORWAY', 'TEHRAN', 'IRAN'), ('OSLO', 'NORWAY', 'TOKYO', 'JAPAN'), ('OSLO', 'NORWAY', 'ATHENS', 'GREECE'), ('OSLO', 'NORWAY', 'BAGHDAD', 'IRAQ'), ('OSLO', 'NORWAY', 'BANGKOK', 'THAILAND'), ('OSLO', 'NORWAY', 'BEIJING', 'CHINA'), ('OSLO', 'NORWAY', 'BERLIN', 'GERMANY'), ('OSLO', 'NORWAY', 'BERN', 'SWITZERLAND'), ('OSLO', 'NORWAY', 'CAIRO', 'EGYPT'), ('OSLO', 'NORWAY', 'CANBERRA', 'AUSTRALIA'), ('OSLO', 'NORWAY', 'HANOI', 'VIETNAM'), ('OSLO', 'NORWAY', 'HAVANA', 'CUBA'), ('OSLO', 'NORWAY', 'HELSINKI', 'FINLAND'), ('OSLO', 'NORWAY', 'ISLAMABAD', 'PAKISTAN'), ('OSLO', 'NORWAY', 'KABUL', 'AFGHANISTAN'), ('OSLO', 'NORWAY', 'LONDON', 'ENGLAND'), ('OSLO', 'NORWAY', 'MADRID', 'SPAIN'), ('OSLO', 'NORWAY', 'MOSCOW', 'RUSSIA'), ('OTTAWA', 'CANADA', 'PARIS', 'FRANCE'), ('OTTAWA', 'CANADA', 'ROME', 'ITALY'), ('OTTAWA', 'CANADA', 'STOCKHOLM', 'SWEDEN'), ('OTTAWA', 'CANADA', 'TEHRAN', 'IRAN'), ('OTTAWA', 'CANADA', 'TOKYO', 'JAPAN'), ('OTTAWA', 'CANADA', 'ATHENS', 'GREECE'), ('OTTAWA', 'CANADA', 'BANGKOK', 'THAILAND'), ('OTTAWA', 'CANADA', 'BEIJING', 'CHINA'), ('OTTAWA', 'CANADA', 'BERLIN', 'GERMANY'), ('OTTAWA', 'CANADA', 'BERN', 'SWITZERLAND'), ('OTTAWA', 'CANADA', 'CAIRO', 'EGYPT'), ('OTTAWA', 'CANADA', 'CANBERRA', 'AUSTRALIA'), ('OTTAWA', 'CANADA', 'HANOI', 'VIETNAM'), ('OTTAWA', 'CANADA', 'HAVANA', 'CUBA'), ('OTTAWA', 'CANADA', 'HELSINKI', 'FINLAND'), ('OTTAWA', 'CANADA', 'ISLAMABAD', 'PAKISTAN'), ('OTTAWA', 'CANADA', 'KABUL', 'AFGHANISTAN'), ('OTTAWA', 'CANADA', 'MADRID', 'SPAIN'), ('OTTAWA', 'CANADA', 'MOSCOW', 'RUSSIA'), ('OTTAWA', 'CANADA', 'OSLO', 'NORWAY'), ('PARIS', 'FRANCE', 'ROME', 'ITALY'), ('PARIS', 'FRANCE', 'STOCKHOLM', 'SWEDEN'), ('PARIS', 'FRANCE', 'TEHRAN', 'IRAN'), ('PARIS', 'FRANCE', 'TOKYO', 'JAPAN'), ('PARIS', 'FRANCE', 'ATHENS', 'GREECE'), ('PARIS', 'FRANCE', 'BAGHDAD', 'IRAQ'), ('PARIS', 'FRANCE', 'BANGKOK', 'THAILAND'), ('PARIS', 'FRANCE', 'BEIJING', 'CHINA'), ('PARIS', 'FRANCE', 'BERLIN', 'GERMANY'), ('PARIS', 'FRANCE', 'BERN', 'SWITZERLAND'), ('PARIS', 'FRANCE', 'CAIRO', 'EGYPT'), ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'), ('PARIS', 'FRANCE', 'HANOI', 'VIETNAM'), ('PARIS', 'FRANCE', 'HAVANA', 'CUBA'), ('PARIS', 'FRANCE', 'HELSINKI', 'FINLAND'), ('PARIS', 'FRANCE', 'ISLAMABAD', 'PAKISTAN'), ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN'), ('PARIS', 'FRANCE', 'LONDON', 'ENGLAND'), ('PARIS', 'FRANCE', 'MADRID', 'SPAIN'), ('PARIS', 'FRANCE', 'MOSCOW', 'RUSSIA'), ('PARIS', 'FRANCE', 'OSLO', 'NORWAY'), ('PARIS', 'FRANCE', 'OTTAWA', 'CANADA'), ('ROME', 'ITALY', 'STOCKHOLM', 'SWEDEN'), ('ROME', 'ITALY', 'TEHRAN', 'IRAN'), ('ROME', 'ITALY', 'TOKYO', 'JAPAN'), ('ROME', 'ITALY', 'ATHENS', 'GREECE'), ('ROME', 'ITALY', 'BAGHDAD', 'IRAQ'), ('ROME', 'ITALY', 'BANGKOK', 'THAILAND'), ('ROME', 'ITALY', 'BEIJING', 'CHINA'), ('ROME', 'ITALY', 'BERLIN', 'GERMANY'), ('ROME', 'ITALY', 'BERN', 'SWITZERLAND'), ('ROME', 'ITALY', 'CAIRO', 'EGYPT'), ('ROME', 'ITALY', 'CANBERRA', 'AUSTRALIA'), ('ROME', 'ITALY', 'HANOI', 'VIETNAM'), ('ROME', 'ITALY', 'HAVANA', 'CUBA'), ('ROME', 'ITALY', 'HELSINKI', 'FINLAND'), ('ROME', 'ITALY', 'ISLAMABAD', 'PAKISTAN'), ('ROME', 'ITALY', 'KABUL', 'AFGHANISTAN'), ('ROME', 'ITALY', 'LONDON', 'ENGLAND'), ('ROME', 'ITALY', 'MADRID', 'SPAIN'), ('ROME', 'ITALY', 'MOSCOW', 'RUSSIA'), ('ROME', 'ITALY', 'OSLO', 'NORWAY'), ('ROME', 'ITALY', 'OTTAWA', 'CANADA'), ('ROME', 'ITALY', 'PARIS', 'FRANCE'), ('STOCKHOLM', 'SWEDEN', 'TEHRAN', 'IRAN'), ('STOCKHOLM', 'SWEDEN', 'TOKYO', 'JAPAN'), ('STOCKHOLM', 'SWEDEN', 'ATHENS', 'GREECE'), ('STOCKHOLM', 'SWEDEN', 'BAGHDAD', 'IRAQ'), ('STOCKHOLM', 'SWEDEN', 'BANGKOK', 'THAILAND'), ('STOCKHOLM', 'SWEDEN', 'BEIJING', 'CHINA'), ('STOCKHOLM', 'SWEDEN', 'BERLIN', 'GERMANY'), ('STOCKHOLM', 'SWEDEN', 'BERN', 'SWITZERLAND'), ('STOCKHOLM', 'SWEDEN', 'CAIRO', 'EGYPT'), ('STOCKHOLM', 'SWEDEN', 'CANBERRA', 'AUSTRALIA'), ('STOCKHOLM', 'SWEDEN', 'HANOI', 'VIETNAM'), ('STOCKHOLM', 'SWEDEN', 'HAVANA', 'CUBA'), ('STOCKHOLM', 'SWEDEN', 'HELSINKI', 'FINLAND'), ('STOCKHOLM', 'SWEDEN', 'ISLAMABAD', 'PAKISTAN'), ('STOCKHOLM', 'SWEDEN', 'KABUL', 'AFGHANISTAN'), ('STOCKHOLM', 'SWEDEN', 'LONDON', 'ENGLAND'), ('STOCKHOLM', 'SWEDEN', 'MADRID', 'SPAIN'), ('STOCKHOLM', 'SWEDEN', 'MOSCOW', 'RUSSIA'), ('STOCKHOLM', 'SWEDEN', 'OSLO', 'NORWAY'), ('STOCKHOLM', 'SWEDEN', 'OTTAWA', 'CANADA'), ('STOCKHOLM', 'SWEDEN', 'PARIS', 'FRANCE'), ('STOCKHOLM', 'SWEDEN', 'ROME', 'ITALY'), ('TEHRAN', 'IRAN', 'TOKYO', 'JAPAN'), ('TEHRAN', 'IRAN', 'ATHENS', 'GREECE'), ('TEHRAN', 'IRAN', 'BAGHDAD', 'IRAQ'), ('TEHRAN', 'IRAN', 'BANGKOK', 'THAILAND'), ('TEHRAN', 'IRAN', 'BEIJING', 'CHINA'), ('TEHRAN', 'IRAN', 'BERLIN', 'GERMANY'), ('TEHRAN', 'IRAN', 'CAIRO', 'EGYPT'), ('TEHRAN', 'IRAN', 'HANOI', 'VIETNAM'), ('TEHRAN', 'IRAN', 'HAVANA', 'CUBA'), ('TEHRAN', 'IRAN', 'HELSINKI', 'FINLAND'), ('TEHRAN', 'IRAN', 'ISLAMABAD', 'PAKISTAN'), ('TEHRAN', 'IRAN', 'KABUL', 'AFGHANISTAN'), ('TEHRAN', 'IRAN', 'MOSCOW', 'RUSSIA'), ('TEHRAN', 'IRAN', 'OSLO', 'NORWAY'), ('TEHRAN', 'IRAN', 'OTTAWA', 'CANADA'), ('TEHRAN', 'IRAN', 'PARIS', 'FRANCE'), ('TEHRAN', 'IRAN', 'ROME', 'ITALY'), ('TEHRAN', 'IRAN', 'STOCKHOLM', 'SWEDEN'), ('TOKYO', 'JAPAN', 'ATHENS', 'GREECE'), ('TOKYO', 'JAPAN', 'BAGHDAD', 'IRAQ'), ('TOKYO', 'JAPAN', 'BANGKOK', 'THAILAND'), ('TOKYO', 'JAPAN', 'BEIJING', 'CHINA'), ('TOKYO', 'JAPAN', 'BERLIN', 'GERMANY'), ('TOKYO', 'JAPAN', 'BERN', 'SWITZERLAND'), ('TOKYO', 'JAPAN', 'CAIRO', 'EGYPT'), ('TOKYO', 'JAPAN', 'CANBERRA', 'AUSTRALIA'), ('TOKYO', 'JAPAN', 'HANOI', 'VIETNAM'), ('TOKYO', 'JAPAN', 'HAVANA', 'CUBA'), ('TOKYO', 'JAPAN', 'HELSINKI', 'FINLAND'), ('TOKYO', 'JAPAN', 'ISLAMABAD', 'PAKISTAN'), ('TOKYO', 'JAPAN', 'KABUL', 'AFGHANISTAN'), ('TOKYO', 'JAPAN', 'MADRID', 'SPAIN'), ('TOKYO', 'JAPAN', 'MOSCOW', 'RUSSIA'), ('TOKYO', 'JAPAN', 'OSLO', 'NORWAY'), ('TOKYO', 'JAPAN', 'OTTAWA', 'CANADA'), ('TOKYO', 'JAPAN', 'PARIS', 'FRANCE'), ('TOKYO', 'JAPAN', 'ROME', 'ITALY'), ('TOKYO', 'JAPAN', 'STOCKHOLM', 'SWEDEN'), ('TOKYO', 'JAPAN', 'TEHRAN', 'IRAN')]\n"
     ]
    }
   ],
   "source": [
    "# Test the accuracy of this model\n",
    "analogy_scores = model.wv.evaluate_word_analogies(datapath('questions-words.txt'))\n",
    "\n",
    "print('country - capital corrects', analogy_scores[1][0]['correct'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "mSXz9ajzHQO-",
    "outputId": "fd69b953-dfed-415c-b187-e2a149924ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-29 10:42:01--  https://s3-eu-west-1.amazonaws.com/text-mining-course/sentiment_corpus.zip\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.98.123\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.98.123|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 65051537 (62M) [application/zip]\n",
      "Saving to: ‘sentiment_corpus.zip.1’\n",
      "\n",
      "sentiment_corpus.zi 100%[===================>]  62.04M  17.0MB/s    in 3.6s    \n",
      "\n",
      "2019-10-29 10:42:05 (17.0 MB/s) - ‘sentiment_corpus.zip.1’ saved [65051537/65051537]\n",
      "\n",
      "Archive:  sentiment_corpus.zip\n",
      "replace sentiment_X_trn.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: sentiment_X_trn.npy     \n",
      "replace sentiment_X_tst.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: sentiment_X_tst.npy     \n",
      "replace sentiment_y_trn.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: sentiment_y_trn.npy     \n",
      "replace sentiment_y_tst.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: sentiment_y_tst.npy     \n"
     ]
    }
   ],
   "source": [
    "# use in a model\n",
    "import numpy as np\n",
    "\n",
    "# Load data. Sentiment model in movies reviews.\n",
    "# Reference: http://www.aclweb.org/anthology/P11-1015 \n",
    "\n",
    "! wget https://s3-eu-west-1.amazonaws.com/text-mining-course/sentiment_corpus.zip\n",
    "! unzip sentiment_corpus.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "McLVesfBLVWN"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_trn = np.load(os.path.join(data_path, 'sentiment_X_trn.npy'), allow_pickle=True) \n",
    "X_tst = np.load(os.path.join(data_path, 'sentiment_X_tst.npy'), allow_pickle=True)\n",
    "y_trn = np.load(os.path.join(data_path, 'sentiment_y_trn.npy'), allow_pickle=True) # 1: pos, 0:neg\n",
    "y_tst = np.load(os.path.join(data_path, 'sentiment_y_tst.npy'), allow_pickle=True) # 1: pos, 0:neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "3NvLOMWzHQPB",
    "outputId": "46f77818-378d-41d2-968d-bc65c048b840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "[list(['I', 'had', 'high', 'expectations', 'of', 'this', 'movie', '(', 'the', 'title', ',', 'translated', ',', 'is', '``', 'How', 'We', 'Get', 'Rid', 'of', 'the', 'Others', \"''\", ')', '.', 'After', 'all', ',', 'the', 'concept', 'is', 'great', ':', 'a', 'near', 'future', 'in', 'which', 'the', 'ruling', 'elite', 'has', 'taken', 'the', 'consequence', 'of', 'the', 'right-wing', 'government', \"'s\", 'constant', 'verbal', 'and', 'legislative', 'persecution', 'of', 'so-called', 'freeloaders', 'and', 'the', 'left', 'wing', 'in', 'general', ',', 'and', 'decided', 'to', 'just', 'kill', 'off', 'everyone', 'who', 'can', 'not', 'prove', 'that', 'they', \"'re\", 'contributing', 'something', 'to', 'the', 'establishment', '(', 'the', 'establishment', 'being', 'called', '``', 'the', 'common', 'good', \"''\", ',', 'but', 'actually', 'meaning', 'the', 'interests', 'of', 'the', 'ruling', 'capitalist', 'ideology', ')', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Very', 'cool', 'idea', '!', 'Ideal', 'for', 'biting', 'satire', '!', 'Only', ',', 'this', 'movie', 'completely', 'blows', 'its', 'chance', '.', 'The', 'satire', 'comes', 'out', 'only', 'in', 'a', 'few', 'scenes', 'and', 'performances', 'of', 'absurdity', ',', 'but', 'this', 'satire', 'is', 'not', 'sustained', ';', 'it', 'is', 'neither', 'sharp', 'nor', 'witty', '.', 'And', 'for', 'an', 'alleged', 'comedy', ',', 'the', 'movie', 'has', 'nearly', 'no', 'funny', 'scenes', '.', 'The', 'comedy', ',', 'I', 'assume', ',', 'is', 'supposed', 'to', 'be', 'in', 'the', 'absurdity', 'of', 'the', 'situations', ',', 'but', 'the', 'situations', 'are', 'largely', 'uncomfortable', 'and', 'over-serious', ',', 'rather', 'than', 'evoking', 'either', 'laughter', 'or', 'thought.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'The', 'script', 'is', 'rife', 'with', 'grave', 'errors', 'in', 'disposition', '.', 'The', 'action', 'should', 'have', 'focused', 'on', 'the', 'political', 'aspects', 'and', 'how', 'wrong', 'it', 'would', 'be', 'to', 'do', 'such', 'a', 'thing', ',', 'but', 'instead', 'oodles', 'of', 'time', 'are', 'spent', 'on', 'a', 'young', 'woman', 'who', 'was', 'the', 'one', 'that', 'wrote', 'the', 'new', 'laws', 'for', 'fun', ',', 'and', 'who', \"'s\", 'trying', 'to', 'save', 'everybody', ',', 'by', 'organizing', 'a', 'resistance', 'that', 'ships', 'people', 'to', 'Africa', '.', 'All', 'this', 'is', 'beside', 'the', 'point', '!', 'A', 'movie', 'like', 'this', 'should', 'not', 'pretend', 'to', 'be', 'so', 'serious', '!', 'It', \"'s\", 'a', 'satire', '!', 'A', 'political', 'statement', '.', 'But', 'it', 'does', \"n't\", 'even', 'begin', 'to', 'actually', 'address', 'the', 'problem', 'it', \"'s\", 'supposed', 'to', 'be', 'about', '.', 'Maybe', 'it', 'was', 'afraid', 'of', 'going', 'too', 'far', '?', 'How', 'cowardly', '.', 'That', \"'s\", 'not', 'art', '.', 'It', \"'s\", 'not', 'even', 'real', 'satire.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Søren', 'Pilmark', ',', 'a', 'very', 'serious', 'and', 'by', 'now', 'one', 'of', 'Denmark', \"'s\", 'absolutely', 'senior', 'actors', ',', 'was', 'very', 'good', '.', 'He', 'largely', 'carried', 'what', 'little', 'entertainment', 'value', 'the', 'movie', 'had', '.', 'Everybody', 'else', ':', 'nothing', 'special', '(', 'well', ',', 'perhaps', 'except', 'for', 'Lene', 'Poulsen', ',', 'who', 'did', 'supply', 'a', 'convincing', 'performance', ')', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'In', 'fact', ',', 'a', 'problem', 'with', 'most', 'Danish', 'movies', 'is', 'that', 'the', 'language', 'never', 'sounds', 'natural', '.', 'Neither', 'the', 'formulation', 'nor', 'the', 'delivery', '.', 'Why', 'is', 'it', 'so', 'difficult', 'to', 'make', 'it', 'sound', 'right', '?', 'Why', 'must', 'it', 'be', 'so', 'stilted', 'and', 'artificial', '?', 'I', 'hope', ',', 'when', 'people', 'look', 'at', 'these', 'movies', 'fifty', 'years', 'from', 'now', ',', 'they', 'do', \"n't\", 'think', 'that', 'this', 'was', 'how', 'people', 'talked', 'in', 'general', 'Danish', 'society.', '<', 'br', '/', '>', '<', 'br', '/', '>', '3', 'out', 'of', '10', '.'])\n",
      " list(['Way', 'back', 'when', ',', 'the', 'X-Files', 'was', 'an', 'intelligent', ',', 'thought-provoking', 'show', '.', 'A', 'big', 'part', 'of', 'its', 'appeal', 'was', 'that', 'the', 'writers', 'looked', 'to', 'folklore', 'and', 'science', 'for', 'their', 'ideas', ',', 'tying', 'the', 'plot', 'to', 'the', 'spooky', 'side', 'of', 'real', 'life.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'was', 'incredibly', 'wary', 'of', 'the', '8th', 'season', 'when', 'it', 'aired', '.', 'The', 'show', 'had', 'already', 'provided', 'two', 'perfectly', 'good', 'episodes', 'to', 'bow', 'out', 'on', '(', '``', 'One', 'Son', \"''\", 'and', '``', 'Requiem', \"''\", ')', ',', 'and', 'the', '7th', 'season', 'had', 'seen', 'a', 'sharp', 'rise', 'in', 'episodes', 'that', 'scraped', 'the', 'barrel', 'for', 'ideas', 'that', 'were', 'far-fetched', ',', 'implausible', ',', 'or', 'downright', 'silly', '.', 'But', 'I', 'figured', ',', 'hey', ',', 'give', 'it', 'the', 'benefit', 'of', 'a', 'doubt', ',', 'maybe', 'they', \"'re\", 'bringing', 'it', 'back', 'because', 'they', \"'ve\", 'got', 'some', 'great', 'ideas', 'lined', 'up.', '<', 'br', '/', '>', '<', 'br', '/', '>', \"''\", 'Roadrunners', \"''\", 'really', 'was', 'upsetting', '.', 'Following', '``', 'Patience', \"''\", ',', 'which', 'at', 'least', 'offered', 'an', 'interesting', 'angle', 'on', 'the', 'vampire', 'folklore', 'that', 'the', 'show', 'had', 'done', 'well', 'to', 'avoid', ',', 'the', 'episode', 'sees', 'a', 'strange', '(', 'alien', '?', ')', 'parasitic', 'slug', 'with', 'the', 'power', 'of', 'mind', 'control', 'worshipped', 'by', 'a', 'cult', 'of', 'backwoods', 'Christians', '.', 'Oh', ',', 'and', 'they', 'think', 'it', \"'s\", 'the', 'second', 'coming', 'of', 'Christ', ',', 'but', 'you', 'only', 'find', 'that', 'out', 'in', 'the', 'last', 'couple', 'of', 'minutes', '.', 'Seriously', '.', 'There', \"'s\", 'never', '*any*', 'attempt', 'to', 'make', 'sense', 'of', 'this', ',', 'to', 'explain', 'what', 'the', 'slug', 'is', ',', 'why', 'anything', 'that', \"'s\", 'happening', 'is', 'happening', ',', 'or', 'anything', '.', 'Even', 'in', 'the', 'show', \"'s\", 'early', 'years', '-', 'in', 'fact', ',', '*especially*', 'then', '-', 'you', 'could', 'expect', 'a', 'little', 'bit', 'more', 'depth', ',', 'a', 'bit', 'of', 'background', ',', 'or', 'if', 'not', 'that', 'then', 'the', 'opposite', '-', 'a', 'bit', 'of', 'mystery', ',', 'some', 'uncertainty', 'about', 'what', 'this', 'was', 'all', 'about', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'It', \"'s\", 'Scully', 'that', 'really', 'kills', 'it', 'though', '.', 'You', 'could', 'put', 'up', 'with', 'the', 'silliness', 'of', 'the', 'premise', ',', 'but', 'to', 'have', 'a', 'character', 'who', 'has', 'been', 'developed', 'over', 'a', 'good', '7', 'years', 'as', 'a', 'rational', 'skeptic', 'transformed', 'into', 'a', 'gullible', 'maverick', 'purely', 'for', 'the', 'sake', 'of', 'advancing', 'the', 'plot', 'is', 'bizarre', 'to', 'watch', '.', 'You', 'feel', 'like', 'you', \"'re\", 'watching', 'some', 'godawful', 'teen', 'horror', ',', 'except', 'that', 'it', \"'s\", 'a', 'woman', 'well', 'into', 'her', 'thirties', 'throwing', 'herself', 'into', 'the', 'kind', 'of', 'creepy', 'isolated', 'community', 'that', 'she', \"'s\", 'spent', 'the', 'best', 'part', 'of', 'a', 'decade', 'uncovering', 'the', 'sinister', 'underbelly', 'of', ',', 'being', 'either', 'outwitted', 'by', 'very', 'stereotypical', 'hicks', 'or', 'utterly', 'indifferent', 'to', 'her', 'own', 'safety', '.', 'Oh', ',', 'and', 'by', 'the', 'way', ',', 'Doggett', ',', 'the', 'new', 'Mudler', ',', 'is', \"n't\", 'around', '.', 'Scully', 'just', 'wandered', 'off', 'into', 'the', 'desert', 'to', 'look', 'into', 'a', 'brutal', 'murder', 'on', 'her', 'own', 'without', 'him', '.', 'He', 'shows', 'up', 'at', 'the', 'end', 'to', 'save', 'the', 'day', '-', 'I', 'ca', \"n't\", 'even', 'remember', 'why', '-', 'but', 'apart', 'from', 'that', ',', 'he', \"'s\", 'not', 'really', 'in', 'it', '.', 'Again', ':', 'seriously.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'In', 'short', ',', 'it', 'feels', 'like', 'either', 'a', 'generic', 'script', 'written', 'for', 'another', 'show', ',', 'or', 'someone', \"'s\", 'pet', 'movie', 'project', 'which', 'they', \"'ve\", 'been', 'allowed', 'to', 'shove', 'like', 'a', 'mutant', 'leech', 'into', 'the', 'spine', 'of', 'an', 'existing', ',', 'long-running', 'show', 'at', 'a', 'time', 'when', 'it', 'was', 'at', 'its', 'most', 'vulnerable', '.', 'It', 'might', \"'ve\", 'worked', 'on', 'a', 'lesser', 'show', ',', 'where', 'the', 'characters', 'are', 'more', 'archetypal', 'and', 'the', 'audience', 'expects', 'less', '.', 'But', 'The', 'X-Files', 'had', 'a', 'good', 'thing', 'going', ',', 'and', 'Scully', 'was', 'one', 'of', 'the', 'strongest', 'and', 'most', 'idiosyncratic', 'TV', 'characters', 'of', 'the', '90s', '.', 'Deciding', 'that', 'you', \"'re\", 'going', 'to', 'change', 'her', 'personality', 'for', 'the', 'sake', 'of', 'a', 'story', 'that', 'they', 'must', \"'ve\", 'done', 'on', 'Star', 'Trek', 'a', 'good', 'fifty', 'or', 'so', 'times', 'is', 'pointless', '.'])]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_trn.shape)\n",
    "print(X_trn[:2])\n",
    "print(y_trn[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "97XtN7zzYiGc",
    "outputId": "56dc6559-6808-45ac-e7a8-993d24de6e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '</s>', 'one', 'zero', 'of', 'and', 'two', 'to', 'in', 'a']\n"
     ]
    }
   ],
   "source": [
    "# Check the 'stype' of the keys of embeddings\n",
    "vocab = list(model.vocab.keys())\n",
    "print(vocab[:10])\n",
    "\n",
    "# In this case are lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Grn4Uw5dHQPE"
   },
   "outputs": [],
   "source": [
    "# Represent each sentence by the average embeding of the words located in the embedings dictionary\n",
    "\n",
    "def encode_text(corpus, model):\n",
    "    '''\n",
    "    Function to encode text sentences into one embedding by sentence\n",
    "        input: A list of sentences (corpus) and a embeddings model (model)\n",
    "        output: One embedding for each sentence (average of embeddings of the words in the sentence)\n",
    "    '''\n",
    "    features_list = []\n",
    "    for s in corpus:\n",
    "        features = []\n",
    "        for t in s:\n",
    "            if str.lower(t) in model.vocab.keys():\n",
    "                features += [model[str.lower(t)]]\n",
    "        features_list += [np.mean(features, axis=0)] \n",
    "    return np.array(features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SXd0NMaGHQPI",
    "outputId": "527ed9a7-9697-4fa3-b23c-77edfe270745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeds trn shape: (25000, 300)\n",
      "Embeds tst shape: (25000, 300)\n"
     ]
    }
   ],
   "source": [
    "# Check embedings shape\n",
    "embeds_trn = encode_text(X_trn, model)\n",
    "print('Embeds trn shape:', embeds_trn.shape)\n",
    "\n",
    "embeds_tst = encode_text(X_tst, model)\n",
    "print('Embeds tst shape:', embeds_tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dd0aL9_THQPL",
    "outputId": "048ac180-4256-4331-b3bf-899ec6e59c02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8382\n"
     ]
    }
   ],
   "source": [
    "# Build a model and evaluate it\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Train\n",
    "text_clf_svm = LinearSVC()\n",
    "text_clf_svm.fit(embeds_trn, y_trn)\n",
    "\n",
    "#Evaluate test data\n",
    "predicted = text_clf_svm.predict(embeds_tst)\n",
    "print('Test accuracy:', np.mean(predicted == y_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rk_EDPEgHQPN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aui8vqQZHQPP"
   },
   "source": [
    "# Word embedings in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iiYPsRtqLjFh"
   },
   "outputs": [],
   "source": [
    "# Install modelo with embeddings\n",
    "! python -m spacy download en_core_web_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-E3INiT3HQPP"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a model with embedings . The small models don't have embedings\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "print('Model loaded!')\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdMUScSJHQPR"
   },
   "outputs": [],
   "source": [
    "# Chech if the token has a vector.\n",
    "tokens = nlp(u'dog cat banana bibliopole')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DV9Ow_R3HQPT"
   },
   "outputs": [],
   "source": [
    "# For a given document, calculate similarity between 'apples' and 'oranges' and 'boots' and 'hippos'\n",
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "apples = doc[0]\n",
    "oranges = doc[2]\n",
    "boots = doc[6]\n",
    "hippos = doc[8]\n",
    "print('Similarity between words:')\n",
    "print('apples vs oranges: ', apples.similarity(oranges))\n",
    "print('boots vs hippos:', boots.similarity(hippos))\n",
    "\n",
    "print()\n",
    "print('Similarity between a word and a sentence:')\n",
    "# Print similarity between sentence and word 'fruit'\n",
    "apples_sent, boots_sent = doc.sents\n",
    "fruit = doc.vocab[u'fruit']\n",
    "print('apples sentence vs fruit word: ', apples_sent.similarity(fruit))\n",
    "print('boots sentence vs fruit word:', boots_sent.similarity(fruit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nuax6xwDHQPV"
   },
   "outputs": [],
   "source": [
    "# Show a vector\n",
    "apples.vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZa0faF1HQPX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_gensim_word2vect.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
