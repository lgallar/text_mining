{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRz44zmBP-ka"
   },
   "source": [
    "# Model4: Stacked LSTM\n",
    "    - Preprocess data.\n",
    "    - Prepare sequences to model.\n",
    "    - Build model.\n",
    "    - Validate it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rtORWJT9QIu6"
   },
   "outputs": [],
   "source": [
    "# Configure to use tensorboard in colab\n",
    "\n",
    "#CPU\n",
    "#!pip install -q tensorflow==2.0.0\n",
    "\n",
    "#GPU\n",
    "!pip install -q tensorflow-gpu==2.0.0\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuzIvmFVP-kb"
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xLYPqpbvP-kc"
   },
   "outputs": [],
   "source": [
    "# Header\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print('Tensorflow version: ', tf.__version__)\n",
    "\n",
    "\n",
    "#Show images\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOuQMf-YQfV6"
   },
   "outputs": [],
   "source": [
    "# link drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9g9BbPbWP-kh"
   },
   "outputs": [],
   "source": [
    "# Import train and test data\n",
    "save_path= './gdrive/My Drive/text_mining'\n",
    "\n",
    "X_train = np.load(os.path.join(save_path, 'X_train.npy'))\n",
    "y_train = np.load(os.path.join(save_path, 'y_train.npy'))\n",
    "X_test  = np.load(os.path.join(save_path, 'X_test.npy'))\n",
    "y_test  = np.load(os.path.join(save_path, 'y_test.npy'))\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Q_skL_7P-kk"
   },
   "source": [
    "## Prepare sequences to model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xR2t88WhP-kk"
   },
   "outputs": [],
   "source": [
    "max_features = 20000 # Number of most frequent words selected. the less frequent recode to 0\n",
    "max_len = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "\n",
    "#Select the most frequent max_features, recode others using 0\n",
    "def remove_features(x):\n",
    "    return [[0 if w >= max_features else w for w in sen] for sen in x]\n",
    "\n",
    "X_train = remove_features(X_train)\n",
    "X_test  = remove_features(X_test)\n",
    "\n",
    "\n",
    "# Cut or complete the sentences to length = maxlen\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eakz0eoCP-kn"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-tVZgVeP-ko"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "dim_embedings = 128 #Dimension of the embedings vector\n",
    "num_hidden_rnn = 128 #Num of neurons in the Recurent network \n",
    "\n",
    "print('Build model 1 - Basic model...')\n",
    "\n",
    "# LAYER 1: inputs\n",
    "seq_prev_input = tf.keras.layers.Input(shape=(max_len, ), dtype='int32') \n",
    "\n",
    "\n",
    "\n",
    "# LAYER 2: Create embedings\n",
    "embeds = tf.keras.layers.Embedding(max_features, dim_embedings, input_length=max_len)(seq_prev_input)\n",
    "\n",
    "# LAYERS 3: RNN - forwards LSTM with dropout\n",
    "droput_percent = 0.3\n",
    "\n",
    "forwards1 = tf.keras.layers.LSTM(num_hidden_rnn, return_sequences=True,\n",
    "                 dropout=droput_percent, recurrent_dropout=droput_percent, name='Forward_1')(embeds)\n",
    "\n",
    "\n",
    "forwards2   = tf.keras.layers.LSTM(num_hidden_rnn, return_sequences=False,\n",
    "                 dropout=droput_percent, recurrent_dropout=droput_percent, name='Forward_2')(forwards1)\n",
    "\n",
    "\n",
    "\n",
    "# LAYER 4: Dense layer to outputs - softmax activation\n",
    "output = tf.keras.layers.Dense(2, activation='softmax')(forwards2)\n",
    "\n",
    "\n",
    "\n",
    "# Model Architecture defined\n",
    "model_1 = tf.keras.models.Model(inputs=seq_prev_input, outputs=output)\n",
    "model_1.summary()\n",
    "\n",
    "# Compile model and select optimizer\n",
    "rms_optimizer = tf.keras.optimizers.RMSprop(lr=0.001)\n",
    "model_1.compile(loss='sparse_categorical_crossentropy', optimizer=rms_optimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Getm7PHmP-kq"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "batch_size = 128\n",
    "\n",
    "print(\"Train...\")\n",
    "tbCallBack = tf.keras.callbacks.TensorBoard(log_dir='./tensorboard/sentiment/StakedLSTM')\n",
    "history = model_1.fit(X_train, y_train, batch_size=batch_size, epochs=5,\n",
    "                      validation_data=(X_test, y_test), callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sXAfAR_hQ6VB"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "\n",
    "# Start tensorboard\n",
    "%tensorboard --logdir ./tensorboard/sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiB9GrhVP-kt"
   },
   "outputs": [],
   "source": [
    "#Plot graphs in the notebook output\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HxG7qgF9P-kw"
   },
   "source": [
    "## Validate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lGogvz6P-kx"
   },
   "outputs": [],
   "source": [
    "# Score and obtain probabilities\n",
    "pred_test = model_1.predict(X_test)\n",
    "print(pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQcXc5BGP-kz"
   },
   "outputs": [],
   "source": [
    "#Import metrics\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "#Calculate accuracy with sklearn\n",
    "print(accuracy_score(y_test, [1 if p>0.5 else 0 for p in pred_test[:,1]]))\n",
    "\n",
    "#Calculate ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, pred_test[:,1])\n",
    "print('AUC: ', auc(fpr, tpr)  )\n",
    "\n",
    "#Plot ROC curve\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "juGgFSLmP-k2"
   },
   "outputs": [],
   "source": [
    "# Score new text\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Load dictionary\n",
    "import pickle\n",
    "with open(os.path.join(save_path,'worddict.pickle'), 'rb') as pfile:\n",
    "    worddict = pickle.load(pfile)\n",
    "\n",
    "    \n",
    "def tokenize(sentences):\n",
    "    from nltk import word_tokenize\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens += [word_tokenize(sentence)]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def generate_sequence(sentences, dictionary):\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in ss]\n",
    "    return seqs\n",
    "\n",
    "\n",
    "def remove_features(x):\n",
    "    return [[0 if w >= max_features else w for w in sen] for sen in x]\n",
    "\n",
    "\n",
    "def score_new_text(text):\n",
    "    seq = generate_sequence(tokenize([text]), worddict)\n",
    "    seq = remove_features(seq)\n",
    "    seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_len)\n",
    "    pred_test = model_1.predict(seq, batch_size=1)\n",
    "    return float(pred_test[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i0Opia2mP-k4"
   },
   "outputs": [],
   "source": [
    "#Evaluate one negative record\n",
    "\n",
    "text = \"You have to start worrying when you see that Michael Madsen is leading the Cast of any movie. I wont go through the list of shame that is his movie career.<br /><br />I watched 45 minutes and still was not sure what really was going on. The movie consisted of a love hate relationship between Madsen and Argento, Which basically was Madsen insulting her, threatening violence and generally treating her like dirt. She on the other hand loves him, then shes doesn't, then she does, the she desires him, then she loves him again......whats wrong with you woman !!!! <br /><br />The Script is awful, lousy soundtrack and pointless aggressive and crude sexuality which i believe was added to entice some viewers as the movie has little else to offer. I would have given the movie a 1 but it just about managed a 2 with a little excitement in the last 20 minutes. It did actually answer one question in the final few minutes but i am not going to share that, i will make you suffer for the full movie like i did.\"\n",
    "print(text)\n",
    "print('Positive score:', score_new_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYIqxtWfP-k6"
   },
   "outputs": [],
   "source": [
    "#Evaluate one positive record\n",
    "\n",
    "text = \"The distribution was good, the subject could have been interessant and comic. whereas, he described the wandering of an old non credible communist looking for loving sensations. Instead of this, the atmosphere is nor lively nor heavy.\"\n",
    "print(text)\n",
    "print('Positive score:', score_new_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTFomEQJUsRh"
   },
   "source": [
    "# Nueva sección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-elMLEslP-k9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "04_Model4_Stacked LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
