{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0f0eY_Fzi4GB"
   },
   "source": [
    "# Identify tags in airline database\n",
    "\n",
    "## Deep LSTM networks\n",
    "\n",
    "    - Bidirectional LSTM\n",
    "    - Stacked LSTM\n",
    "    - Bidirectional stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0iU6cO6WtNX"
   },
   "outputs": [],
   "source": [
    "# Configure to use tensorboard in colab\n",
    "\n",
    "#CPU\n",
    "#!pip install -q tensorflow==2.0.0\n",
    "\n",
    "#GPU\n",
    "#!pip install -q tensorflow-gpu==2.0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmSjTX6Vi4GC"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "print(tf.__version__)\n",
    "\n",
    "#Show images\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plt configuration\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mnlPS5vhi4GI"
   },
   "source": [
    "## Dataset transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9uQSflNi4GI"
   },
   "outputs": [],
   "source": [
    "# download data\n",
    "! wget https://s3-eu-west-1.amazonaws.com/text-mining-course/atis.zip\n",
    "! unzip atis.zip\n",
    "\n",
    "# Read data\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "atis_file = './atis/atis.pkl'\n",
    "\n",
    "with open(atis_file,'rb') as f:\n",
    "    if sys.version_info.major==2:\n",
    "        train, test, dicts = pickle.load(f) #python2.7\n",
    "    else:\n",
    "        train, test, dicts = pickle.load(f, encoding='bytes') #python3\n",
    "\n",
    "#Dictionaries and train test partition\n",
    "w2idx, ne2idx, labels2idx = dicts[b'words2idx'], dicts[b'tables2idx'], dicts[b'labels2idx']\n",
    "    \n",
    "idx2w  = dict((v,k) for k,v in w2idx.items())\n",
    "idx2la = dict((v,k) for k,v in labels2idx.items())\n",
    "\n",
    "train_x, _, train_label = train\n",
    "test_x,  _,  test_label  = test\n",
    "\n",
    "\n",
    "# Max value of word coding to assign the ID_PAD\n",
    "ID_PAD = np.max([np.max(tx) for tx in train_x]) + 1\n",
    "print('ID_PAD: ', ID_PAD)\n",
    "\n",
    "def context(l, size=3):\n",
    "    l = list(l)\n",
    "    lpadded = size // 2 * [ID_PAD] + l + size // 2 * [ID_PAD]\n",
    "    out = [lpadded[i:(i + size)] for i in range(len(l))]\n",
    "    return out\n",
    "\n",
    "\n",
    "#Â Create train and test X y.\n",
    "X_trn=[]\n",
    "for s in train_x:\n",
    "    X_trn += context(s,size=10)\n",
    "X_trn = np.array(X_trn)\n",
    "\n",
    "X_tst=[]\n",
    "for s in test_x:\n",
    "    X_tst += context(s,size=10)\n",
    "X_tst = np.array(X_tst)\n",
    "\n",
    "print('X trn shape: ', X_trn.shape)\n",
    "print('X_tst shape: ',X_tst.shape)\n",
    "\n",
    "\n",
    "y_trn=[]\n",
    "for s in train_label:\n",
    "    y_trn += list(s)\n",
    "y_trn = np.array(y_trn)\n",
    "print('y_trn shape: ',y_trn.shape)\n",
    "\n",
    "y_tst=[]\n",
    "for s in test_label:\n",
    "    y_tst += list(s)\n",
    "y_tst = np.array(y_tst)\n",
    "print('y_tst shape: ',y_tst.shape)\n",
    "\n",
    "\n",
    "print('Num labels: ',len(set(y_trn)))\n",
    "print('Num words: ',len(set(idx2w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sP-WZxtYi4GL"
   },
   "outputs": [],
   "source": [
    "# data attributes\n",
    "input_seq_length = X_trn.shape[1]\n",
    "input_vocabulary_size = len(set(idx2w)) + 1\n",
    "output_length = 127\n",
    "\n",
    "#Model parameters\n",
    "embedding_size=64\n",
    "num_hidden_lstm = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Vqa8sDTi4GQ"
   },
   "source": [
    "# Bidirectional LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r2D7Y20Bi4GR"
   },
   "outputs": [],
   "source": [
    "# build the model: Simple LSTM with embedings\n",
    "print('Build model 1')\n",
    "seq_input = tf.keras.layers.Input(shape=([input_seq_length]), name='prev') \n",
    "    \n",
    "embeds = tf.keras.layers.Embedding(input_vocabulary_size, embedding_size)(seq_input)\n",
    "\n",
    "# Bidirectional LSTM\n",
    "forwards  = tf.keras.layers.LSTM(num_hidden_lstm, return_sequences=False, name='Forward')(embeds)\n",
    "backwards = tf.keras.layers.LSTM(num_hidden_lstm, return_sequences=False, go_backwards=True, name='Backward')(embeds)\n",
    "rnn_out = tf.keras.layers.concatenate([forwards, backwards], axis=-1, name='Forward_Backward')\n",
    "\n",
    "output = tf.keras.layers.Dense(output_length, activation='softmax')(rnn_out)\n",
    "\n",
    "model1 = tf.keras.models.Model(inputs=seq_input, outputs=output)\n",
    "model1.summary()\n",
    "\n",
    "# Optimizer\n",
    "adam_optimizer = tf.keras.optimizers.Adam()\n",
    "model1.compile(loss='sparse_categorical_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "\n",
    "#Fit model\n",
    "tb_callback_bilstm = tf.keras.callbacks.TensorBoard(log_dir='./tensorboard/airline/BILSTM/')\n",
    "history1 = model1.fit(X_trn, y_trn, batch_size=128, epochs=20,\n",
    "                      validation_data=(X_tst, y_tst), callbacks=[tb_callback_bilstm])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWaM54-vi4GU"
   },
   "source": [
    "# Stacked LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9atpCBKi4GU"
   },
   "outputs": [],
   "source": [
    "# build the model: Simple LSTM with embedings\n",
    "print('Build model 1')\n",
    "seq_input = tf.keras.layers.Input(shape=([input_seq_length]), name='prev') \n",
    "    \n",
    "embeds = tf.keras.layers.Embedding(input_vocabulary_size, embedding_size)(seq_input)\n",
    "\n",
    "# Stacked LSTM\n",
    "forwards1 = tf.keras.layers.LSTM(num_hidden_lstm, return_sequences=True,  name='Forward1')(embeds)\n",
    "forwards2 = tf.keras.layers.LSTM(num_hidden_lstm, return_sequences=False, name='Forward2')(forwards1)\n",
    "\n",
    "output = tf.keras.layers.Dense(output_length, activation='softmax')(forwards2)\n",
    "\n",
    "model2 = tf.keras.models.Model(inputs=seq_input, outputs=output)\n",
    "model2.summary()\n",
    "\n",
    "# Optimizer\n",
    "adam_optimizer = tf.keras.optimizers.Adam()\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Fit model\n",
    "tb_callback_lstm2 = tf.keras.callbacks.TensorBoard(log_dir='./tensorboard/airline/LSTM2/')\n",
    "history2 = model2.fit(X_trn, y_trn, batch_size=128, epochs=20,\n",
    "                      validation_data=(X_tst, y_tst), callbacks=[tb_callback_lstm2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFgfOxUBi4GZ"
   },
   "source": [
    "# Bidirectional stacked LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JsGmsIANi4GZ"
   },
   "outputs": [],
   "source": [
    "# build the model: Bidirectional stacked LSTM with embedings\n",
    "print('Build model 1')\n",
    "seq_input = tf.keras.layers.Input(shape=([input_seq_length]), name='prev') \n",
    "    \n",
    "embeds = tf.keras.layers.Embedding(input_vocabulary_size, embedding_size)(seq_input)\n",
    "\n",
    "# Bidirectional LSTM 1\n",
    "forwards1  = tf.keras.layers.LSTM(num_hidden_lstm, return_sequences=True, name='Forward1')(embeds)\n",
    "backwards1 = tf.keras.layers.LSTM(num_hidden_lstm, return_sequences=True, go_backwards=True, name='Backward1')(embeds)\n",
    "rnn_out1 = tf.keras.layers.concatenate([forwards1, backwards1], axis=-1, name='Forward_Backward1')\n",
    "\n",
    "# Bidirectional LSTM 1\n",
    "forwards2  = tf.keras.layers.LSTM(num_hidden_lstm, return_sequences=False, name='Forward2')(rnn_out1)\n",
    "backwards2 = tf.keras.layers.LSTM(num_hidden_lstm, return_sequences=False, go_backwards=True, name='Backward2')(rnn_out1)\n",
    "rnn_out2 = tf.keras.layers.concatenate([forwards2, backwards2], axis=-1, name='Forward_Backward2')\n",
    "\n",
    "\n",
    "output = tf.keras.layers.Dense(output_length, activation='softmax')(rnn_out2)\n",
    "\n",
    "model3 = tf.keras.models.Model(inputs=seq_input, outputs=output)\n",
    "model3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "89vWWk2JWU3t"
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "adam_optimizer = tf.keras.optimizers.Adam()\n",
    "model3.compile(loss='sparse_categorical_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Fit model\n",
    "tb_callback_bilstm2 = tf.keras.callbacks.TensorBoard(log_dir='./tensorboard/airline/BILSTM2/')\n",
    "history3 = model3.fit(X_trn, y_trn, batch_size=128, epochs=20,\n",
    "                      validation_data=(X_tst, y_tst), callbacks=[tb_callback_bilstm2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZl9hzNZi4Ge"
   },
   "outputs": [],
   "source": [
    "# Compare validation of the 3 models\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "\n",
    "plt.plot(history1.history['val_acc'], label='BLSTM')\n",
    "plt.plot(history2.history['val_acc'], label='Stacked LSTM')\n",
    "plt.plot(history3.history['val_acc'], label='Stacked BLSTM')\n",
    "#plt.plot(history1.history['val_accuracy'], label='BLSTM') # tf 2.0\n",
    "#plt.plot(history2.history['val_accuracy'], label='Stacked LSTM') # tf 2.0\n",
    "#plt.plot(history3.history['val_accuracy'], label='Stacked BLSTM') # tf 2.0\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2KXxbCVLi4Gk"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard\n",
    "\n",
    "# Start tensorboard\n",
    "%tensorboard --logdir ./tensorboard/airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SfHE8rlW6zP"
   },
   "source": [
    "## Retrieve the learned embeddings to show into the tensorflow projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFCwI3WRYHz7"
   },
   "outputs": [],
   "source": [
    "# List layers to identigy the embedding layer\n",
    "model2.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3O_N5zFn4RP"
   },
   "outputs": [],
   "source": [
    "# Get the trined weights of the embedding layer\n",
    "model_layers = model2.layers[1]\n",
    "weights = model_layers.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7YqszM_5YbE3"
   },
   "outputs": [],
   "source": [
    "idx2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TrURD_ytW1Tl"
   },
   "outputs": [],
   "source": [
    "# Generate files to load into the projector\n",
    "\n",
    "import io\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in idx2w.keys():\n",
    "    word = idx2w[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(str(word) + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-9qm05uzW1Xl"
   },
   "outputs": [],
   "source": [
    "# download to local\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download('vecs.tsv')\n",
    "    files.download('meta.tsv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Muod8Y6kXLOl"
   },
   "source": [
    "## Load the files into the projector\n",
    "\n",
    "http://projector.tensorflow.org/\n",
    "\n",
    "Execute t-sne and examine results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LCDyfez2XSjs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "02_identify_tags_in_airline_database_BLSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
