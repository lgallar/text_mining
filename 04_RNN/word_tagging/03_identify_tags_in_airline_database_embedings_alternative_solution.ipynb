{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "03_identify_tags_in_airline_database_embedings_alternative_solution.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCwgfxXktuFn"
      },
      "source": [
        "# Identify tags in airline database\n",
        "\n",
        "## Pending to complete the data creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MCu31zwtuFo"
      },
      "source": [
        "# Alternative solution\n",
        "\n",
        "## Architecture\n",
        "    - Two inputs: \n",
        "        previous words seq_inputf = [W_i-k, ..., W_i-1, W_i] \n",
        "        Next words seq_inputb = [W_i, W_i+1, ...,W_i+k]\n",
        "    - Two embeding layers with shared weights\n",
        "    - Two LSTM, one forward and one backward\n",
        "    - A final concatenation and a dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKewyKbLtuFp",
        "outputId": "a65222fb-0c7c-45b4-e416-25c5e9a32d4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import os \n",
        "import numpy as np \n",
        "import tensorflow as tf \n",
        "print(tf.__version__)\n",
        "\n",
        "#Show images\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# plt configuration\n",
        "plt.rcParams['figure.figsize'] = (10, 10)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRZzbMh-tuFu",
        "outputId": "bca56211-8584-4c2a-c8c7-552d549c1ff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# download data\n",
        "! wget https://s3-eu-west-1.amazonaws.com/text-mining-course/atis.zip\n",
        "! unzip atis.zip\n",
        "\n",
        "# Read data\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "atis_file = './atis/atis.pkl'\n",
        "\n",
        "with open(atis_file,'rb') as f:\n",
        "    if sys.version_info.major==2:\n",
        "        train, test, dicts = pickle.load(f) #python2.7\n",
        "    else:\n",
        "        train, test, dicts = pickle.load(f, encoding='bytes') #python3\n",
        "\n",
        "#Dictionaries and train test partition\n",
        "w2idx, ne2idx, labels2idx = dicts[b'words2idx'], dicts[b'tables2idx'], dicts[b'labels2idx']\n",
        "    \n",
        "idx2w  = dict((v,k) for k,v in w2idx.items())\n",
        "idx2la = dict((v,k) for k,v in labels2idx.items())\n",
        "\n",
        "train_x, _, train_label = train\n",
        "test_x,  _,  test_label  = test\n",
        "\n",
        "\n",
        "# Max value of word coding to assign the ID_PAD\n",
        "ID_PAD = np.max([np.max(tx) for tx in train_x]) + 1\n",
        "print('ID_PAD: ', ID_PAD)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-09 09:35:43--  https://s3-eu-west-1.amazonaws.com/text-mining-course/atis.zip\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.41.139\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.41.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1483513 (1.4M) [application/zip]\n",
            "Saving to: ‘atis.zip’\n",
            "\n",
            "atis.zip            100%[===================>]   1.41M  2.32MB/s    in 0.6s    \n",
            "\n",
            "2020-10-09 09:35:45 (2.32 MB/s) - ‘atis.zip’ saved [1483513/1483513]\n",
            "\n",
            "Archive:  atis.zip\n",
            "   creating: atis/\n",
            "  inflating: atis/atis.fold0.pkl     \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/atis/\n",
            "  inflating: __MACOSX/atis/._atis.fold0.pkl  \n",
            "  inflating: atis/atis.fold1.pkl     \n",
            "  inflating: __MACOSX/atis/._atis.fold1.pkl  \n",
            "  inflating: atis/atis.fold2.pkl     \n",
            "  inflating: __MACOSX/atis/._atis.fold2.pkl  \n",
            "  inflating: atis/atis.fold3.pkl     \n",
            "  inflating: __MACOSX/atis/._atis.fold3.pkl  \n",
            "  inflating: atis/atis.fold4.pkl     \n",
            "  inflating: __MACOSX/atis/._atis.fold4.pkl  \n",
            "  inflating: atis/atis.pkl           \n",
            "  inflating: __MACOSX/atis/._atis.pkl  \n",
            "ID_PAD:  572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIqT4340tuFx",
        "outputId": "7de3d4a9-6479-438e-8fdf-20f7566e357d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "def context(l, size=3):\n",
        "    l = list(l)\n",
        "    lpadded = size * [ID_PAD] + l + size * [ID_PAD]\n",
        "    out_f = [lpadded[(i - size):i+1] for i in range(size, size + len(l), 1)]\n",
        "    out_b = [lpadded[i:(i + size)+1] for i in range(size, size + len(l), 1)]\n",
        "    return out_f, out_b\n",
        "\n",
        "context([1,2,3], size=3)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[572, 572, 572, 1], [572, 572, 1, 2], [572, 1, 2, 3]],\n",
              " [[1, 2, 3, 572], [2, 3, 572, 572], [3, 572, 572, 572]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txQFbzpwtuF0",
        "outputId": "884f2f18-922d-45e3-abd9-f193621ee4c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "def context(l, size=3):\n",
        "    l = list(l)\n",
        "    lpadded = size * [ID_PAD] + l + size * [ID_PAD]\n",
        "    out_f = [lpadded[(i - size):i+1] for i in range(size, size + len(l), 1)]\n",
        "    out_b = [lpadded[i:(i + size)+1] for i in range(size, size + len(l), 1)]\n",
        "    return out_f, out_b\n",
        "\n",
        "\n",
        "input_seq_length = 11\n",
        "\n",
        "# Create train and test X y.\n",
        "X_trn_f = []\n",
        "X_trn_b = []\n",
        "for s in train_x:\n",
        "    out_f, out_b = context(s, size=input_seq_length-1)\n",
        "    X_trn_f += out_f\n",
        "    X_trn_b += out_b\n",
        "X_trn_f = np.array(X_trn_f)\n",
        "X_trn_b = np.array(X_trn_b)\n",
        "\n",
        "X_tst_f = []\n",
        "X_tst_b = []\n",
        "for s in test_x:\n",
        "    out_f, out_b = context(s,size=10)\n",
        "    X_tst_f += out_f\n",
        "    X_tst_b += out_b\n",
        "X_tst_f = np.array(X_tst_f)\n",
        "X_tst_b = np.array(X_tst_b)\n",
        "\n",
        "print('X trn shape: ', X_trn_f.shape, X_trn_b.shape)\n",
        "print('X_tst shape: ', X_tst_f.shape, X_tst_b.shape)\n",
        "\n",
        "\n",
        "y_trn=[]\n",
        "for s in train_label:\n",
        "    y_trn += list(s)\n",
        "y_trn = np.array(y_trn)\n",
        "print('y_trn shape: ',y_trn.shape)\n",
        "\n",
        "y_tst=[]\n",
        "for s in test_label:\n",
        "    y_tst += list(s)\n",
        "y_tst = np.array(y_tst)\n",
        "print('y_tst shape: ',y_tst.shape)\n",
        "\n",
        "\n",
        "print('Num labels: ',len(set(y_trn)))\n",
        "print('Num words: ',len(set(idx2w)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X trn shape:  (56590, 11) (56590, 11)\n",
            "X_tst shape:  (9198, 11) (9198, 11)\n",
            "y_trn shape:  (56590,)\n",
            "y_tst shape:  (9198,)\n",
            "Num labels:  121\n",
            "Num words:  572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32OMKu6xtuF3"
      },
      "source": [
        "# data attributes\n",
        "input_seq_length = X_trn_f.shape[1]\n",
        "input_vocabulary_size = len(set(idx2w)) + 1\n",
        "output_length = 127\n",
        "\n",
        "#Model parameters\n",
        "embedding_size=64\n",
        "num_hidden_lstm = 128\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY2hVc6ztuF6",
        "outputId": "6bf5325a-4354-41d9-d889-42e2cebb4ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# build the model: Simple LSTM with embedings\n",
        "print('Build model 1')\n",
        "seq_input_f = tf.keras.layers.Input(shape=([input_seq_length]), name='prev1') \n",
        "seq_input_b = tf.keras.layers.Input(shape=([input_seq_length]), name='prev2') \n",
        "    \n",
        "embeds = tf.keras.layers.Embedding(input_vocabulary_size, embedding_size)\n",
        "\n",
        "embeds_f = embeds(seq_input_f)\n",
        "embeds_b = embeds(seq_input_b)\n",
        "\n",
        "lstm_f = tf.keras.layers.LSTM(128)(embeds_f)\n",
        "lstm_b = tf.keras.layers.LSTM(128, go_backwards=True)(embeds_b)\n",
        "\n",
        "concat = tf.keras.layers.concatenate([lstm_f, lstm_b], axis=-1)\n",
        "\n",
        "    \n",
        "output = tf.keras.layers.Dense(output_length, activation='softmax')(concat)\n",
        "\n",
        "\n",
        "model1 = tf.keras.models.Model(inputs=[seq_input_f, seq_input_b], outputs=output)\n",
        "model1.summary()\n",
        "\n",
        "# Optimizer\n",
        "adam_optimizer = tf.keras.optimizers.Adam()\n",
        "model1.compile(loss='sparse_categorical_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model 1\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "prev1 (InputLayer)              [(None, 11)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "prev2 (InputLayer)              [(None, 11)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 11, 64)       36672       prev1[0][0]                      \n",
            "                                                                 prev2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 128)          98816       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 128)          98816       embedding[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 256)          0           lstm[0][0]                       \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 127)          32639       concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 266,943\n",
            "Trainable params: 266,943\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xsk1DnQXtuF8",
        "outputId": "34049475-3b77-4e1d-ca67-b20008c37f9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "#Fit model\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir='./tensorboard/airline/shared_embedings/')\n",
        "\n",
        "history = model1.fit([X_trn_f, X_trn_b], y_trn, batch_size=128, epochs=20,\n",
        "           validation_data=([X_tst_f, X_tst_b], y_tst), callbacks=[tb_callback])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "  1/443 [..............................] - ETA: 0s - loss: 4.8434 - accuracy: 0.0000e+00WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "443/443 [==============================] - 34s 77ms/step - loss: 1.1702 - accuracy: 0.7624 - val_loss: 0.5850 - val_accuracy: 0.8833\n",
            "Epoch 2/20\n",
            "443/443 [==============================] - 34s 76ms/step - loss: 0.2892 - accuracy: 0.9382 - val_loss: 0.2409 - val_accuracy: 0.9534\n",
            "Epoch 3/20\n",
            "443/443 [==============================] - 34s 77ms/step - loss: 0.1269 - accuracy: 0.9735 - val_loss: 0.1798 - val_accuracy: 0.9666\n",
            "Epoch 4/20\n",
            "443/443 [==============================] - 34s 76ms/step - loss: 0.0757 - accuracy: 0.9847 - val_loss: 0.1459 - val_accuracy: 0.9743\n",
            "Epoch 5/20\n",
            "443/443 [==============================] - 34s 76ms/step - loss: 0.0535 - accuracy: 0.9882 - val_loss: 0.1424 - val_accuracy: 0.9758\n",
            "Epoch 6/20\n",
            "443/443 [==============================] - 34s 76ms/step - loss: 0.0397 - accuracy: 0.9910 - val_loss: 0.1348 - val_accuracy: 0.9745\n",
            "Epoch 7/20\n",
            "443/443 [==============================] - 34s 76ms/step - loss: 0.0311 - accuracy: 0.9927 - val_loss: 0.1270 - val_accuracy: 0.9770\n",
            "Epoch 8/20\n",
            "443/443 [==============================] - 34s 76ms/step - loss: 0.0239 - accuracy: 0.9946 - val_loss: 0.1277 - val_accuracy: 0.9776\n",
            "Epoch 9/20\n",
            "443/443 [==============================] - 34s 76ms/step - loss: 0.0196 - accuracy: 0.9948 - val_loss: 0.1442 - val_accuracy: 0.9756\n",
            "Epoch 10/20\n",
            "443/443 [==============================] - 33s 75ms/step - loss: 0.0160 - accuracy: 0.9963 - val_loss: 0.1388 - val_accuracy: 0.9779\n",
            "Epoch 11/20\n",
            "443/443 [==============================] - 33s 75ms/step - loss: 0.0124 - accuracy: 0.9970 - val_loss: 0.1494 - val_accuracy: 0.9747\n",
            "Epoch 12/20\n",
            "443/443 [==============================] - 34s 76ms/step - loss: 0.0101 - accuracy: 0.9975 - val_loss: 0.1388 - val_accuracy: 0.9780\n",
            "Epoch 13/20\n",
            "443/443 [==============================] - 34s 76ms/step - loss: 0.0086 - accuracy: 0.9978 - val_loss: 0.1349 - val_accuracy: 0.9779\n",
            "Epoch 14/20\n",
            "443/443 [==============================] - 34s 77ms/step - loss: 0.0062 - accuracy: 0.9986 - val_loss: 0.1492 - val_accuracy: 0.9784\n",
            "Epoch 15/20\n",
            "443/443 [==============================] - 34s 77ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.1440 - val_accuracy: 0.9792\n",
            "Epoch 16/20\n",
            "443/443 [==============================] - 34s 77ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.1437 - val_accuracy: 0.9786\n",
            "Epoch 17/20\n",
            "443/443 [==============================] - 34s 77ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.1461 - val_accuracy: 0.9787\n",
            "Epoch 18/20\n",
            "443/443 [==============================] - 35s 80ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.1622 - val_accuracy: 0.9779\n",
            "Epoch 19/20\n",
            "443/443 [==============================] - 34s 77ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.1487 - val_accuracy: 0.9778\n",
            "Epoch 20/20\n",
            "443/443 [==============================] - 34s 77ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.1682 - val_accuracy: 0.9773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Syv5Jv7EtuF_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}